{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from ast import literal_eval\n",
    "from collections import Counter\n",
    "from scipy.optimize import fmin_l_bfgs_b\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "from gensim.models import Doc2Vec\n",
    "import multiprocessing\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import optimizeTopicVectors as ot\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# hoseong's class\n",
    "from STMD import *\n",
    "from ASUM import *\n",
    "from ASUM_Embedding import *\n",
    "from preprocess import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('D:/Dropbox/2016-2/졸업논문/STMD/data/SentiWords-0.txt','r') as f:\n",
    "    pos = f.readlines()\n",
    "pos_seed = [word.strip() for word in pos]\n",
    "with open('D:/Dropbox/2016-2/졸업논문/STMD/data/SentiWords-1.txt','r') as f:\n",
    "    neg = f.readlines()\n",
    "neg_seed = [word.strip() for word in neg]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load data\n",
    "work_path = \"/media/hs-ubuntu/data/dataset/MasterThesis/STMD_data/\"\n",
    "# work_path = \"E:/dataset/MasterThesis/STMD_data/\"\n",
    "\n",
    "data = pd.read_csv(work_path + \"preprocess_complete_Electronics.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewTime</th>\n",
       "      <th>asin</th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>overall</th>\n",
       "      <th>helpful</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>title</th>\n",
       "      <th>brand</th>\n",
       "      <th>reviewSentence</th>\n",
       "      <th>sent_length</th>\n",
       "      <th>reviewSentence_tagged</th>\n",
       "      <th>preprocessed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2013-07-21</td>\n",
       "      <td>B00CM0XHNS</td>\n",
       "      <td>A372YX80GGM7DR</td>\n",
       "      <td>5.0</td>\n",
       "      <td>576</td>\n",
       "      <td>Ok, so I didn't buy this on Amazon, as I didn'...</td>\n",
       "      <td>Ultimate Ears BOOM Wireless Bluetooth Speaker ...</td>\n",
       "      <td>Logitech</td>\n",
       "      <td>[\"Ok, so I didn't buy this on Amazon, as I did...</td>\n",
       "      <td>58</td>\n",
       "      <td>[[('Ok', 'NNP'), (',', ','), ('so', 'IN'), ('I...</td>\n",
       "      <td>[['ok', 'not_buy', 'amazon', 'not_want', 'wait...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2013-05-19</td>\n",
       "      <td>B00BQ5RY1G</td>\n",
       "      <td>A1BG2Z071TYO7P</td>\n",
       "      <td>2.0</td>\n",
       "      <td>522</td>\n",
       "      <td>I received a Harmony Ultimate from Logitech be...</td>\n",
       "      <td>Logitech Harmony Ultimate Remote with Customiz...</td>\n",
       "      <td>Logitech</td>\n",
       "      <td>['I received a Harmony Ultimate from Logitech ...</td>\n",
       "      <td>27</td>\n",
       "      <td>[[('I', 'PRP'), ('received', 'VBD'), ('a', 'DT...</td>\n",
       "      <td>[['receiv', 'harmoni', 'ultim', 'logitech', 's...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2013-12-16</td>\n",
       "      <td>B00EZ9XG62</td>\n",
       "      <td>AELAESM03451</td>\n",
       "      <td>1.0</td>\n",
       "      <td>290</td>\n",
       "      <td>This review is for the iPad Air keyboard. I ha...</td>\n",
       "      <td>Logitech Ultrathin Keyboard Cover for iPad Air...</td>\n",
       "      <td>Logitech</td>\n",
       "      <td>['This review is for the iPad Air keyboard.', ...</td>\n",
       "      <td>23</td>\n",
       "      <td>[[('This', 'DT'), ('review', 'NN'), ('is', 'VB...</td>\n",
       "      <td>[['review', 'ipad', 'air', 'keyboard'], ['keyb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2013-01-21</td>\n",
       "      <td>B0099SMFVQ</td>\n",
       "      <td>A36CMGR5ELUM34</td>\n",
       "      <td>5.0</td>\n",
       "      <td>283</td>\n",
       "      <td>Design: Very well put together. Elegant and th...</td>\n",
       "      <td>Logitech Bluetooth Illuminated Keyboard K810 f...</td>\n",
       "      <td>Logitech</td>\n",
       "      <td>['Design: Very well put together.', 'Elegant a...</td>\n",
       "      <td>28</td>\n",
       "      <td>[[('Design', 'NN'), (':', ':'), ('Very', 'RB')...</td>\n",
       "      <td>[['design', 'veri', 'well', 'put', 'togeth'], ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2013-07-29</td>\n",
       "      <td>B00CM0XHNS</td>\n",
       "      <td>A9TETE58A7JR3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>260</td>\n",
       "      <td>So, I've been testing a few bluetooth speakers...</td>\n",
       "      <td>Ultimate Ears BOOM Wireless Bluetooth Speaker ...</td>\n",
       "      <td>Logitech</td>\n",
       "      <td>[\"So, I've been testing a few bluetooth speake...</td>\n",
       "      <td>57</td>\n",
       "      <td>[[('So', 'RB'), (',', ','), ('I', 'PRP'), (\"'v...</td>\n",
       "      <td>[['test', 'bluetooth', 'speaker', 'week'], ['a...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   reviewTime        asin      reviewerID  overall  helpful  \\\n",
       "0  2013-07-21  B00CM0XHNS  A372YX80GGM7DR      5.0      576   \n",
       "1  2013-05-19  B00BQ5RY1G  A1BG2Z071TYO7P      2.0      522   \n",
       "2  2013-12-16  B00EZ9XG62    AELAESM03451      1.0      290   \n",
       "3  2013-01-21  B0099SMFVQ  A36CMGR5ELUM34      5.0      283   \n",
       "4  2013-07-29  B00CM0XHNS   A9TETE58A7JR3      3.0      260   \n",
       "\n",
       "                                          reviewText  \\\n",
       "0  Ok, so I didn't buy this on Amazon, as I didn'...   \n",
       "1  I received a Harmony Ultimate from Logitech be...   \n",
       "2  This review is for the iPad Air keyboard. I ha...   \n",
       "3  Design: Very well put together. Elegant and th...   \n",
       "4  So, I've been testing a few bluetooth speakers...   \n",
       "\n",
       "                                               title     brand  \\\n",
       "0  Ultimate Ears BOOM Wireless Bluetooth Speaker ...  Logitech   \n",
       "1  Logitech Harmony Ultimate Remote with Customiz...  Logitech   \n",
       "2  Logitech Ultrathin Keyboard Cover for iPad Air...  Logitech   \n",
       "3  Logitech Bluetooth Illuminated Keyboard K810 f...  Logitech   \n",
       "4  Ultimate Ears BOOM Wireless Bluetooth Speaker ...  Logitech   \n",
       "\n",
       "                                      reviewSentence  sent_length  \\\n",
       "0  [\"Ok, so I didn't buy this on Amazon, as I did...           58   \n",
       "1  ['I received a Harmony Ultimate from Logitech ...           27   \n",
       "2  ['This review is for the iPad Air keyboard.', ...           23   \n",
       "3  ['Design: Very well put together.', 'Elegant a...           28   \n",
       "4  [\"So, I've been testing a few bluetooth speake...           57   \n",
       "\n",
       "                               reviewSentence_tagged  \\\n",
       "0  [[('Ok', 'NNP'), (',', ','), ('so', 'IN'), ('I...   \n",
       "1  [[('I', 'PRP'), ('received', 'VBD'), ('a', 'DT...   \n",
       "2  [[('This', 'DT'), ('review', 'NN'), ('is', 'VB...   \n",
       "3  [[('Design', 'NN'), (':', ':'), ('Very', 'RB')...   \n",
       "4  [[('So', 'RB'), (',', ','), ('I', 'PRP'), (\"'v...   \n",
       "\n",
       "                                        preprocessed  \n",
       "0  [['ok', 'not_buy', 'amazon', 'not_want', 'wait...  \n",
       "1  [['receiv', 'harmoni', 'ultim', 'logitech', 's...  \n",
       "2  [['review', 'ipad', 'air', 'keyboard'], ['keyb...  \n",
       "3  [['design', 'veri', 'well', 'put', 'togeth'], ...  \n",
       "4  [['test', 'bluetooth', 'speaker', 'week'], ['a...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_sample(data, brand_name, count, ratio = 1, random_state = 42):\n",
    "    brand = data[data['brand']==brand_name]\n",
    "    pos_reviews = brand[brand.overall >= 4]\n",
    "    neg_reviews = brand[brand.overall <= 2]\n",
    "    if ratio == 1:\n",
    "        pos_sample = pos_reviews.sample(count, random_state=random_state)\n",
    "        neg_sample = neg_reviews.sample(count, random_state=random_state)\n",
    "        df = pd.concat([pos_sample, neg_sample], axis=0)\n",
    "    else:\n",
    "        df = brand.sample(count * 2, random_state = random_state)\n",
    "    df['preprocessed'] = df.preprocessed.apply(lambda row: literal_eval(row))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# brand = ['Apple', 'Samsung','Canon']\n",
    "# brand_df = data[data.brand.isin(brand)]\n",
    "# brand_df.reset_index(drop=True, inplace=True)\n",
    "# #긍정, 부정 반반씩\n",
    "# pos_reviews = brand_df[brand_df.overall >= 4]\n",
    "# neg_reviews = brand_df[brand_df.overall <= 2]\n",
    "# pos_sample = pos_reviews.sample(3500, random_state=23)\n",
    "# neg_sample = neg_reviews.sample(3500, random_state=42)\n",
    "# df = pd.concat([pos_sample, neg_sample], axis=0)\n",
    "# df['preprocessed'] = df.preprocessed.apply(lambda row: literal_eval(row))\n",
    "# df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# apple, samsung, canon 1:1:1\n",
    "apple = extract_sample(data, 'Apple', 500, ratio = 1)\n",
    "samsung = extract_sample(data, 'Samsung', 500, ratio = 1)\n",
    "canon = extract_sample(data, 'Canon', 500, ratio = 1)\n",
    "df = pd.concat([apple, samsung, canon], axis=0)\n",
    "df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Apple 리뷰에서만\n",
    "apple = extract_sample(data, 'Apple', 2500, ratio = 0)\n",
    "df = apple.copy()\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "# df.to_csv(work_path + \"apple_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(work_path + 'apple_data.csv' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3307, 12)\n",
      "(1324, 12)\n"
     ]
    }
   ],
   "source": [
    "print(df[df.overall >= 4].shape)\n",
    "print(df[df.overall <= 2].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hs-ubuntu/.local/lib/python3.5/site-packages/gensim/models/phrases.py:248: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n"
     ]
    }
   ],
   "source": [
    "# prepare\n",
    "sentence_list, sentiment_label, sentence_senti_label, \\\n",
    "pos_neg_sentence_indices, pos_neg_sentiment_label, numSentence = prepare(df)\n",
    "\n",
    "documents, sentence_list_again, bigram, documents_label\\\n",
    "= bigram_and_sentence(sentence_senti_label, sentence_list, numSentence, max_vocab=5000, threshold = 5, min_count = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2507014"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "asum = ASUM(pos_seed, neg_seed, numTopics=10, alpha=0.01, beta=0.001, gamma=1, numSentiments=2)\n",
    "asum._initialize_(sentence_list_again, pos_neg_sentence_indices, pos_neg_sentiment_label, sentiment_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "t = [86, 80, 72]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[86, 80, 72]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(work_path + \"BagOfSentences.txt\", 'w') as f:\n",
    "    for doc in range(asum.numDocs):\n",
    "        f.write(str(len(asum.doc_sent_word_dict[doc])))\n",
    "        f.write(\"\\n\")\n",
    "        for sent in asum.doc_sent_word_dict[doc]:\n",
    "            for word in sent:\n",
    "                f.write(str(word) + ' ')\n",
    "            f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[278, 878, 817, 445, 410],\n",
       " [34, 138, 122, 30, 310, 86, 636, 1896, 229, 6, 1415, 122]]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "asum.doc_sent_word_dict[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(work_path + \"BagOfSentences.txt\", 'w') as f:\n",
    "    for sent in sent_index:\n",
    "        f.write(str(len(sent)))\n",
    "        f.write(\"\\n\")\n",
    "        for word in sent:\n",
    "            f.write(str(word) + ' ')\n",
    "        f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "dic = OrderedDict(sorted(asum.word2idx.items(), key=lambda t: t[1]))\n",
    "word_list = []\n",
    "for key in dic.keys():\n",
    "    word_list.append(key)\n",
    "with open(work_path + \"WordList.txt\", 'w') as f:\n",
    "    for word in word_list:\n",
    "        f.writelines(word + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 그냥 ASUM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting iteration 2 of 100\n",
      "0.5615\n",
      "Starting iteration 4 of 100\n",
      "0.567\n",
      "Starting iteration 6 of 100\n",
      "0.5765\n",
      "Starting iteration 8 of 100\n",
      "0.5885\n",
      "Starting iteration 10 of 100\n",
      "0.6\n",
      "Starting iteration 12 of 100\n",
      "0.614\n",
      "Starting iteration 14 of 100\n",
      "0.633\n",
      "Starting iteration 16 of 100\n",
      "0.6365\n",
      "Starting iteration 18 of 100\n",
      "0.6305\n",
      "Starting iteration 20 of 100\n",
      "0.6175\n",
      "Starting iteration 22 of 100\n",
      "0.625\n",
      "Starting iteration 24 of 100\n",
      "0.621\n",
      "Starting iteration 26 of 100\n",
      "0.611\n",
      "Starting iteration 28 of 100\n",
      "0.6215\n",
      "Starting iteration 30 of 100\n",
      "0.615\n",
      "Starting iteration 32 of 100\n",
      "0.611\n",
      "Starting iteration 34 of 100\n",
      "0.608\n",
      "Starting iteration 36 of 100\n",
      "0.609\n",
      "Starting iteration 38 of 100\n",
      "0.602\n",
      "Starting iteration 40 of 100\n",
      "0.6005\n",
      "Starting iteration 42 of 100\n",
      "0.601\n",
      "Starting iteration 44 of 100\n",
      "0.597\n",
      "Starting iteration 46 of 100\n",
      "0.593\n",
      "Starting iteration 48 of 100\n",
      "0.5995\n",
      "Starting iteration 50 of 100\n",
      "0.5975\n",
      "Starting save model\n",
      "Starting iteration 52 of 100\n",
      "0.6\n",
      "Starting iteration 54 of 100\n",
      "0.598\n",
      "Starting iteration 56 of 100\n",
      "0.5945\n",
      "Starting iteration 58 of 100\n",
      "0.5875\n",
      "Starting iteration 60 of 100\n",
      "0.59\n",
      "Starting iteration 62 of 100\n",
      "0.593\n",
      "Starting iteration 64 of 100\n",
      "0.595\n",
      "Starting iteration 66 of 100\n",
      "0.588\n",
      "Starting iteration 68 of 100\n",
      "0.588\n",
      "Starting iteration 70 of 100\n",
      "0.59\n",
      "Starting iteration 72 of 100\n",
      "0.5925\n",
      "Starting iteration 74 of 100\n",
      "0.588\n",
      "Starting iteration 76 of 100\n",
      "0.592\n",
      "Starting iteration 78 of 100\n",
      "0.5895\n",
      "Starting iteration 80 of 100\n",
      "0.589\n",
      "Starting iteration 82 of 100\n",
      "0.586\n",
      "Starting iteration 84 of 100\n",
      "0.591\n",
      "Starting iteration 86 of 100\n",
      "0.586\n",
      "Starting iteration 88 of 100\n",
      "0.5935\n",
      "Starting iteration 90 of 100\n",
      "0.589\n",
      "Starting iteration 92 of 100\n",
      "0.591\n",
      "Starting iteration 94 of 100\n",
      "0.5925\n",
      "Starting iteration 96 of 100\n",
      "0.5875\n",
      "Starting iteration 98 of 100\n",
      "0.587\n",
      "Starting iteration 100 of 100\n",
      "0.591\n",
      "Starting save model\n",
      "Wall time: 8min 29s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "asum_path = \"E:/dataset/MasterThesis/Models/ASUM_test1_\"\n",
    "asum = ASUM(pos_seed, neg_seed, numTopics=10, alpha=0.01, beta=0.001, gamma=1, numSentiments=2)\n",
    "asum._initialize_(sentence_list_again, pos_neg_sentence_indices, pos_neg_sentiment_label, sentiment_label)\n",
    "asum.run(sentence_list_again, save_path=asum_path, print_iter=2, save_iter = 50, maxIters= 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting iteration 2 of 100\n",
      "0.499\n",
      "Starting iteration 4 of 100\n",
      "0.481\n",
      "Starting iteration 6 of 100\n",
      "0.479\n",
      "Starting iteration 8 of 100\n",
      "0.494\n",
      "Starting iteration 10 of 100\n",
      "0.493\n",
      "Starting iteration 12 of 100\n",
      "0.505666666667\n",
      "Starting iteration 14 of 100\n",
      "0.483333333333\n",
      "Starting iteration 16 of 100\n",
      "0.480666666667\n",
      "Starting iteration 18 of 100\n",
      "0.478666666667\n",
      "Starting iteration 20 of 100\n",
      "0.462666666667\n",
      "Starting iteration 22 of 100\n",
      "0.464\n",
      "Starting iteration 24 of 100\n",
      "0.460666666667\n",
      "Starting iteration 26 of 100\n",
      "0.451666666667\n",
      "Starting iteration 28 of 100\n",
      "0.456666666667\n",
      "Starting iteration 30 of 100\n",
      "0.454\n",
      "Starting iteration 32 of 100\n",
      "0.452\n",
      "Starting iteration 34 of 100\n",
      "0.445\n",
      "Starting iteration 36 of 100\n",
      "0.453\n",
      "Starting iteration 38 of 100\n",
      "0.447333333333\n",
      "Starting iteration 40 of 100\n",
      "0.442\n",
      "Starting iteration 42 of 100\n",
      "0.455333333333\n",
      "Starting iteration 44 of 100\n",
      "0.451333333333\n",
      "Starting iteration 46 of 100\n",
      "0.453333333333\n",
      "Starting iteration 48 of 100\n",
      "0.453333333333\n",
      "Starting iteration 50 of 100\n",
      "0.448\n",
      "Starting save model\n",
      "Starting iteration 52 of 100\n",
      "0.444333333333\n",
      "Starting iteration 54 of 100\n",
      "0.451\n",
      "Starting iteration 56 of 100\n",
      "0.450333333333\n",
      "Starting iteration 58 of 100\n",
      "0.454\n",
      "Starting iteration 60 of 100\n",
      "0.447666666667\n",
      "Starting iteration 62 of 100\n",
      "0.449\n",
      "Starting iteration 64 of 100\n",
      "0.444\n",
      "Starting iteration 66 of 100\n",
      "0.453\n",
      "Starting iteration 68 of 100\n",
      "0.448\n",
      "Starting iteration 70 of 100\n",
      "0.448666666667\n",
      "Starting iteration 72 of 100\n",
      "0.445\n",
      "Starting iteration 74 of 100\n",
      "0.446666666667\n",
      "Starting iteration 76 of 100\n",
      "0.446\n",
      "Starting iteration 78 of 100\n",
      "0.447666666667\n",
      "Starting iteration 80 of 100\n",
      "0.445\n",
      "Starting iteration 82 of 100\n",
      "0.451666666667\n",
      "Starting iteration 84 of 100\n",
      "0.447666666667\n",
      "Starting iteration 86 of 100\n",
      "0.443333333333\n",
      "Starting iteration 88 of 100\n",
      "0.448666666667\n",
      "Starting iteration 90 of 100\n",
      "0.452333333333\n",
      "Starting iteration 92 of 100\n",
      "0.448333333333\n",
      "Starting iteration 94 of 100\n",
      "0.448666666667\n",
      "Starting iteration 96 of 100\n",
      "0.454\n",
      "Starting iteration 98 of 100\n",
      "0.449666666667\n",
      "Starting iteration 100 of 100\n",
      "0.454\n",
      "Starting save model\n",
      "Wall time: 16min 33s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "asum_path = \"E:/dataset/MasterThesis/Models/ASUM_test2_\"\n",
    "asum = ASUM(pos_seed, neg_seed, numTopics=10, alpha=0.01, beta=0.001, gamma=1, numSentiments=2)\n",
    "asum._initialize_(sentence_list_again, pos_neg_sentence_indices, pos_neg_sentiment_label, sentiment_label)\n",
    "asum.run(sentence_list_again, save_path=asum_path, print_iter=2, save_iter = 50, maxIters= 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting iteration 2 of 100\n",
      "0.520666666667\n",
      "Starting iteration 4 of 100\n",
      "0.522\n",
      "Starting iteration 6 of 100\n",
      "0.523333333333\n",
      "Starting iteration 8 of 100\n",
      "0.501\n",
      "Starting iteration 10 of 100\n",
      "0.496\n",
      "Starting iteration 12 of 100\n",
      "0.506333333333\n",
      "Starting iteration 14 of 100\n",
      "0.506333333333\n",
      "Starting iteration 16 of 100\n",
      "0.514666666667\n",
      "Starting iteration 18 of 100\n",
      "0.518\n",
      "Starting iteration 20 of 100\n",
      "0.519\n",
      "Starting iteration 22 of 100\n",
      "0.522666666667\n",
      "Starting iteration 24 of 100\n",
      "0.526333333333\n",
      "Starting iteration 26 of 100\n",
      "0.530333333333\n",
      "Starting iteration 28 of 100\n",
      "0.526\n",
      "Starting iteration 30 of 100\n",
      "0.527666666667\n",
      "Starting iteration 32 of 100\n",
      "0.526\n",
      "Starting iteration 34 of 100\n",
      "0.520666666667\n",
      "Starting iteration 36 of 100\n",
      "0.525666666667\n",
      "Starting iteration 38 of 100\n",
      "0.519333333333\n",
      "Starting iteration 40 of 100\n",
      "0.514\n",
      "Starting iteration 42 of 100\n",
      "0.515333333333\n",
      "Starting iteration 44 of 100\n",
      "0.518333333333\n",
      "Starting iteration 46 of 100\n",
      "0.518\n",
      "Starting iteration 48 of 100\n",
      "0.508\n",
      "Starting iteration 50 of 100\n",
      "0.512333333333\n",
      "Starting save model\n",
      "Starting iteration 52 of 100\n",
      "0.516666666667\n",
      "Starting iteration 54 of 100\n",
      "0.512333333333\n",
      "Starting iteration 56 of 100\n",
      "0.513\n",
      "Starting iteration 58 of 100\n",
      "0.51\n",
      "Starting iteration 60 of 100\n",
      "0.508666666667\n",
      "Starting iteration 62 of 100\n",
      "0.512333333333\n",
      "Starting iteration 64 of 100\n",
      "0.509\n",
      "Starting iteration 66 of 100\n",
      "0.512333333333\n",
      "Starting iteration 68 of 100\n",
      "0.513666666667\n",
      "Starting iteration 70 of 100\n",
      "0.508333333333\n",
      "Starting iteration 72 of 100\n",
      "0.503333333333\n",
      "Starting iteration 74 of 100\n",
      "0.508666666667\n",
      "Starting iteration 76 of 100\n",
      "0.500333333333\n",
      "Starting iteration 78 of 100\n",
      "0.504333333333\n",
      "Starting iteration 80 of 100\n",
      "0.502333333333\n",
      "Starting iteration 82 of 100\n",
      "0.497333333333\n",
      "Starting iteration 84 of 100\n",
      "0.493\n",
      "Starting iteration 86 of 100\n",
      "0.496333333333\n",
      "Starting iteration 88 of 100\n",
      "0.494333333333\n",
      "Starting iteration 90 of 100\n",
      "0.497666666667\n",
      "Starting iteration 92 of 100\n",
      "0.499333333333\n",
      "Starting iteration 94 of 100\n",
      "0.496666666667\n",
      "Starting iteration 96 of 100\n",
      "0.494333333333\n",
      "Starting iteration 98 of 100\n",
      "0.493\n",
      "Starting iteration 100 of 100\n",
      "0.489333333333\n",
      "Starting save model\n",
      "Wall time: 16min 52s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "asum_path = \"E:/dataset/MasterThesis/Models/ASUM_test3_\"\n",
    "asum = ASUM(pos_seed, neg_seed, numTopics=10, alpha=0.01, beta=0.001, gamma=1, numSentiments=2)\n",
    "asum._initialize_(sentence_list_again, pos_neg_sentence_indices, pos_neg_sentiment_label, sentiment_label)\n",
    "asum.run(sentence_list_again, save_path=asum_path, print_iter=2, save_iter = 50, maxIters= 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6000"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pos_neg_sentence_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5553"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pos_neg_sentiment_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 1, 0, 0, 1, 0, 1, 0, 0]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_neg_sentiment_label[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
