{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    " \n",
    "class Printer():\n",
    "    \"\"\"\n",
    "    Print things to stdout on one line dynamically\n",
    "    \"\"\"\n",
    "    def __init__(self,data):\n",
    "        sys.stdout.write(\"\\r\\x1b[K\"+data.__str__())\n",
    "        sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start loading Electronics raw data\n",
      "Completed loading Electronics raw data, time : 540\n",
      "Start loading %s meta data Electronics\n",
      "Completed loading Electronics meta data, time : 49\n",
      "Start join raw and meta of Electronics\n",
      "Completed join raw and meta data of Electronics, time :  6\n",
      "Start extract sentences of Electronics\n",
      "Completed extract sentences of Electronics, time : 496\n",
      "Start extract samples from data\n",
      "Completed extract samples of Electronics, time :  3\n",
      "check shape ----------\n",
      "Electronics shape after sampling : 250000, 10\n",
      "Save before pos tagging start\n",
      "Save before pos tagging completed\n",
      "Start pos tag of sentences in Electronics\n",
      "Completed pos-tagging and save of Electronics, time : 1571\n",
      "Start preprocessing in Electronics\n",
      "Completed preprocess and save of Electronics, time : 282\n",
      "Start loading Beauty raw data\n",
      "Completed loading Beauty raw data, time : 144\n",
      "Start loading %s meta data Beauty\n",
      "Completed loading Beauty meta data, time : 33\n",
      "Start join raw and meta of Beauty\n",
      "Completed join raw and meta data of Beauty, time :  3\n",
      "Start extract sentences of Beauty\n",
      "Completed extract sentences of Beauty, time : 204\n",
      "Start extract samples from data\n",
      "Completed extract samples of Beauty, time :  7\n",
      "check shape ----------\n",
      "Beauty shape after sampling : 202181, 10\n",
      "Save before pos tagging start\n",
      "Save before pos tagging completed\n",
      "Start pos tag of sentences in Beauty\n",
      "Completed pos-tagging and save of Beauty, time : 697\n",
      "Start preprocessing in Beauty\n",
      "Completed preprocess and save of Beauty, time : 136\n",
      "Start loading Clothing_Shoes_and_Jewelry raw data\n",
      "Completed loading Clothing_Shoes_and_Jewelry raw data, time : 404\n",
      "Start loading %s meta data Clothing_Shoes_and_Jewelry\n",
      "Completed loading Clothing_Shoes_and_Jewelry meta data, time : 180\n",
      "Start join raw and meta of Clothing_Shoes_and_Jewelry\n",
      "Completed join raw and meta data of Clothing_Shoes_and_Jewelry, time :  7\n",
      "Start extract sentences of Clothing_Shoes_and_Jewelry\n",
      "Completed extract sentences of Clothing_Shoes_and_Jewelry, time : 119\n",
      "Start extract samples from data\n",
      "Completed extract samples of Clothing_Shoes_and_Jewelry, time :  3\n",
      "check shape ----------\n",
      "Clothing_Shoes_and_Jewelry shape after sampling : 178026, 10\n",
      "Save before pos tagging start\n",
      "Save before pos tagging completed\n",
      "Start pos tag of sentences in Clothing_Shoes_and_Jewelry\n",
      "Completed pos-tagging and save of Clothing_Shoes_and_Jewelry, time : 553\n",
      "Start preprocessing in Clothing_Shoes_and_Jewelry\n",
      "Completed preprocess and save of Clothing_Shoes_and_Jewelry, time : 107\n"
     ]
    }
   ],
   "source": [
    "from raw_preprocess_util import *\n",
    "import time\n",
    "\n",
    "\n",
    "data_path = \"/media/hs-ubuntu/data/dataset/Amazon/\"\n",
    "work_path = \"/media/hs-ubuntu/data/dataset/MasterThesis/\"\n",
    "save_path = \"/media/hs-ubuntu/data/dataset/MasterThesis/STMD_data/\"\n",
    "\n",
    "\n",
    "category_list = [\"Electronics\",\"Beauty\",\"Clothing_Shoes_and_Jewelry\"]\n",
    "\n",
    "for category in category_list:\n",
    "    raw_data_path = data_path + \"reviews_\" + category + \".json.gz\"\n",
    "    meta_data_path = data_path + \"meta_\" + category + \".json.gz\"\n",
    "\n",
    "    print(\"Start loading %s raw data\" % category)\n",
    "    start = time.time()\n",
    "    raw_data = load_data(raw_data_path, year=2013)\n",
    "    end = time.time()\n",
    "    print(\"Completed loading %s raw data, time : %2.f\" % (category, end - start))\n",
    "\n",
    "\n",
    "    print(\"Start loading %s meta data\", category)\n",
    "    start = time.time()\n",
    "    meta_data = load_meta(meta_data_path)\n",
    "    end = time.time()\n",
    "    print(\"Completed loading %s meta data, time : %2.f\" % (category, end - start))\n",
    "\n",
    "\n",
    "    print(\"Start join raw and meta of %s\" % category)\n",
    "    start = time.time()\n",
    "    join_data = join_meta_data(raw_data, meta_data)\n",
    "    end = time.time()\n",
    "    print(\"Completed join raw and meta data of %s, time : %2.f\" % (category, end - start))\n",
    "\n",
    "    # NaN 값을 제거하고, 중간에 값을 저장하기 위해 일단 저장 후 다시 불러옴\n",
    "    join_data.to_csv(save_path + \"join_\" + category + \".csv\", index=False)\n",
    "    join_data = pd.read_csv(save_path + \"join_\" + category + \".csv\")\n",
    "\n",
    "    print(\"Start extract sentences of %s\" % category)\n",
    "    start = time.time()\n",
    "    join_data = extract_sentence(join_data)\n",
    "    end = time.time()\n",
    "    print(\"Completed extract sentences of %s, time : %2.f\" % (category, end - start))\n",
    "\n",
    "    print(\"Start extract samples from data\")\n",
    "    start = time.time()\n",
    "    top_brands_df, top_brands_list = top_brands(join_data)\n",
    "    data_final = sample_data(top_brands_df, top_brands_list)\n",
    "    end = time.time()\n",
    "    print(\"Completed extract samples of %s, time : %2.f\" % (category, end - start))\n",
    "\n",
    "    print(\"check shape ----------\")\n",
    "    print(\"%s shape after sampling : %s, %s\"  % (category, data_final.shape[0], data_final.shape[1]))\n",
    "\n",
    "    # 중간 저장\n",
    "    print(\"Save before pos tagging start\")\n",
    "    data_final.to_csv(save_path + \"raw_\" + category + \".csv\", index=False)\n",
    "    print(\"Save before pos tagging completed\")\n",
    "\n",
    "\n",
    "    # 형태소 분석\n",
    "    print(\"Start pos tag of sentences in %s\" % category)\n",
    "    start = time.time()\n",
    "    data_final['reviewSentence_tagged'] = data_final.reviewSentence.apply(sentence_postag)\n",
    "    data_final.to_csv(save_path + \"pos_tagged_\" + category + \".csv\", index=False)\n",
    "    end = time.time()\n",
    "    print(\"Completed pos-tagging and save of %s, time : %2.f\" % (category, end - start))\n",
    "\n",
    "    # 형태소 분석한거에다가 추가 전처리\n",
    "    print(\"Start preprocessing in %s\" % category)\n",
    "    start = time.time()\n",
    "    data_final['preprocessed'] = data_final.reviewSentence_tagged.apply(preprocessing)\n",
    "    data_final.to_csv(save_path + \"preprocess_complete_\" + category + \".csv\", index=False)\n",
    "    end = time.time()\n",
    "    print(\"Completed preprocess and save of %s, time : %2.f\" % (category, end - start))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start preprocessing in Electronics\n"
     ]
    }
   ],
   "source": [
    "# brand2vec용 전처리\n",
    "from raw_preprocess_util import *\n",
    "import time\n",
    "import pickle\n",
    "from ast import literal_eval\n",
    "import nltk\n",
    "\n",
    "data_path = \"/media/hs-ubuntu/data/dataset/Amazon/\"\n",
    "work_path = \"/media/hs-ubuntu/data/dataset/MasterThesis/\"\n",
    "save_path = \"/media/hs-ubuntu/data/dataset/MasterThesis/STMD_data/\"\n",
    "\n",
    "category_list = [\"Electronics\",\"Beauty\",\"Clothing_Shoes_and_Jewelry\"]\n",
    "\n",
    "for category in category_list:\n",
    "    print(\"Start preprocessing in %s\" % category)\n",
    "    start = time.time()\n",
    "    data_final = pd.read_csv(save_path + \"pos_tagged_\" + category + \".csv\")\n",
    "    data_final['preprocessed'] = data_final.reviewSentence_tagged.apply(lambda row: literal_eval(row))\n",
    "    reviewSentence_tagged = data_final.reviewSentence_tagged.values.tolist()\n",
    "    \n",
    "    new_sent_list = []\n",
    "    adjectives = []\n",
    "    total_tokens = []\n",
    "    for i in range(len(reviewSentence_tagged)):\n",
    "        if (i + 1) % 10 == 0:\n",
    "            Printer(i)\n",
    "        sent, adjective, tokens=brand2vec_preprocess(reviewSentence_tagged[i])\n",
    "        new_sent_list.append(sent)\n",
    "        adjectives.extend(adjective)\n",
    "        total_tokens.extend(tokens)\n",
    "    \n",
    "    # save 전체 단어 분포 / 형용사 단어 분포 dictionary\n",
    "    corpus = nltk.Text(total_text)\n",
    "    freq = nltk.FreqDist(corpus)\n",
    "    with open(work_path + 'brand2vec_dist/' + category + '_total_freq_dist.pkl', 'wb') as f:\n",
    "        pickle.dump(freq, f)\n",
    "    total_text = [word for sent in total_tokens for word in sent]\n",
    "    print(len(list(set(total_text))))\n",
    "    \n",
    "    # 형용사 분포\n",
    "    corpus = nltk.Text(adjectives)\n",
    "    freq = nltk.FreqDist(corpus)\n",
    "    with open(work_path + 'brand2vec_dist/' + category + '_total_adjective_dist.pkl', 'wb') as f:\n",
    "        pickle.dump(freq, f)\n",
    "    \n",
    "    data_final.drop(['reviewSentence','reviewSentence_tagged','preprocessed'], inplace=True)\n",
    "    data_final.to_csv(save_path + \"brand2vec_final_\" + category + \".csv\", index=False)\n",
    "    end = time.time()\n",
    "    print(\"Completed preprocess and save of %s, time : %2.f\" % (category, end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
