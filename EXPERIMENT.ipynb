{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from ast import literal_eval\n",
    "from collections import Counter\n",
    "from scipy.optimize import fmin_l_bfgs_b\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "from gensim.models import Doc2Vec\n",
    "import multiprocessing\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import optimizeTopicVectors as ot\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# hoseong's class\n",
    "from STMD import *\n",
    "from ASUM import *\n",
    "from ASUM_Embedding import *\n",
    "from preprocess import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load data\n",
    "# work_path = \"/media/hs-ubuntu/data/dataset/MasterThesis/STMD_data/\"\n",
    "work_path = \"E:/dataset/MasterThesis/STMD_data/\"\n",
    "\n",
    "data = pd.read_csv(work_path + \"preprocess_complete_Electronics.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_sample(data, brand_name, count, ratio = 1, random_state = 42):\n",
    "    brand = data[data['brand']==brand_name]\n",
    "    pos_reviews = brand[brand.overall >= 4]\n",
    "    neg_reviews = brand[brand.overall <= 2]\n",
    "    if ratio == 1:\n",
    "        pos_sample = pos_reviews.sample(count, random_state=random_state)\n",
    "        neg_sample = neg_reviews.sample(count, random_state=random_state)\n",
    "        df = pd.concat([pos_sample, neg_sample], axis=0)\n",
    "    else:\n",
    "        df = brand.sample(count * 2, random_state = random_state)\n",
    "    df['preprocessed'] = df.preprocessed.apply(lambda row: literal_eval(row))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# brand = ['Apple', 'Samsung','Canon']\n",
    "# brand_df = data[data.brand.isin(brand)]\n",
    "# brand_df.reset_index(drop=True, inplace=True)\n",
    "# #긍정, 부정 반반씩\n",
    "# pos_reviews = brand_df[brand_df.overall >= 4]\n",
    "# neg_reviews = brand_df[brand_df.overall <= 2]\n",
    "# pos_sample = pos_reviews.sample(3500, random_state=23)\n",
    "# neg_sample = neg_reviews.sample(3500, random_state=42)\n",
    "# df = pd.concat([pos_sample, neg_sample], axis=0)\n",
    "# df['preprocessed'] = df.preprocessed.apply(lambda row: literal_eval(row))\n",
    "# df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# apple, samsung, canon 1:1:1\n",
    "apple = extract_sample(data, 'Apple', 500, ratio = 1)\n",
    "samsung = extract_sample(data, 'Samsung', 500, ratio = 1)\n",
    "canon = extract_sample(data, 'Canon', 500, ratio = 1)\n",
    "df = pd.concat([apple, samsung, canon], axis=0)\n",
    "df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1500, 12)\n",
      "(1500, 12)\n"
     ]
    }
   ],
   "source": [
    "print(df[df.overall >= 4].shape)\n",
    "print(df[df.overall <= 2].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# prepare\n",
    "sentence_list, sentiment_label, sentence_senti_label, \\\n",
    "pos_neg_sentence_indices, pos_neg_sentiment_label, numSentence = prepare(df)\n",
    "\n",
    "documents, sentence_list_again, bigram, documents_label\\\n",
    "= bigram_and_sentence(sentence_senti_label, sentence_list, numSentence, max_vocab=5000, threshold = 5, min_count = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27123"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gensim_model(documents, window, dimension, epochs, save_path, save_per_epoch = 5, save=True):\n",
    "    model = Doc2Vec(dm=1, dm_mean=1, negative =10,min_count=0, sample=1e-5,\n",
    "                    window=window, size=dimension, \n",
    "                    workers=multiprocessing.cpu_count(), \n",
    "                    alpha=0.025, min_alpha=0.025)\n",
    "    model.build_vocab(documents)\n",
    "    decrease_rate = 0.025 / epochs\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        random.shuffle(documents)\n",
    "        model.train(documents)\n",
    "        model.alpha -= decrease_rate  # decrease the learning rate\n",
    "        model.min_alpha = model.alpha  # fix the learning rate, no decay\n",
    "        if (epoch + 1) % save_per_epoch ==0:\n",
    "            if save:\n",
    "                model.save(save_path + 'model_' + str(window) + '_' + str(dimension) + '_' + str(epoch+1))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 10/10 [01:49<00:00, 11.39s/it]\n"
     ]
    }
   ],
   "source": [
    "# word / sentiment embedding\n",
    "\n",
    "window = [3]\n",
    "size = [100]\n",
    "passes = 10\n",
    "for w in window:\n",
    "    for s in size:\n",
    "        model = Doc2Vec(dm=1, dm_mean=1, negative =10,min_count=0, sample=1e-5,\n",
    "                        window=w, size=s, \n",
    "                        workers=multiprocessing.cpu_count(), \n",
    "                        alpha=0.025, min_alpha=0.025)\n",
    "        model.build_vocab(documents)\n",
    "\n",
    "        for epoch in tqdm(range(passes)):\n",
    "            random.shuffle(documents)\n",
    "            model.train(documents)\n",
    "            model.alpha -= 0.002  # decrease the learning rate\n",
    "            model.min_alpha = model.alpha  # fix the learning rate, no decay\n",
    "#             if (epoch + 1) % 5 ==0:\n",
    "#                 model.save(model_path + 'model_' + str(w) + '_' + str(s) + '_' + str(epoch+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# model = Doc2Vec.load(\"E:/dataset/MasterThesis/gensim_models/test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('defect', 0.9389455318450928),\n",
       " ('seller', 0.9290028810501099),\n",
       " ('refund', 0.9248861074447632),\n",
       " ('warranti', 0.9145373702049255),\n",
       " ('damag', 0.909091055393219),\n",
       " ('advis', 0.9084977507591248),\n",
       " ('broken', 0.9014047980308533),\n",
       " ('fix', 0.9009304046630859),\n",
       " ('error', 0.8979858160018921),\n",
       " ('told', 0.8963349461555481),\n",
       " ('later', 0.8938384056091309),\n",
       " ('call', 0.8929416537284851),\n",
       " ('two_week', 0.8904902338981628),\n",
       " ('repair', 0.8891378045082092),\n",
       " ('said', 0.8886135816574097),\n",
       " ('product', 0.8848811984062195),\n",
       " ('contact', 0.8841660022735596),\n",
       " ('replac', 0.8836409449577332),\n",
       " ('contact_samsung', 0.8832719922065735),\n",
       " ('amazon', 0.8825801610946655)]"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar([model.docvecs['negative']], topn=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('pro', 0.9145333766937256),\n",
       " ('super', 0.9063757061958313),\n",
       " ('fast', 0.9035416841506958),\n",
       " ('graphic', 0.9029198884963989),\n",
       " ('brilliant', 0.8996568918228149),\n",
       " ('size', 0.8944329619407654),\n",
       " ('huge', 0.8903395533561707),\n",
       " ('far', 0.888909101486206),\n",
       " ('portabl', 0.8875069618225098),\n",
       " ('slower', 0.8839425444602966),\n",
       " ('inch', 0.882956326007843),\n",
       " ('steep', 0.8822929263114929),\n",
       " ('excel', 0.8819456696510315),\n",
       " ('larg', 0.8780012726783752),\n",
       " ('handl', 0.87481290102005),\n",
       " ('beauti', 0.8743758797645569),\n",
       " ('touch_screen', 0.874349057674408),\n",
       " ('great', 0.8740013241767883),\n",
       " ('crisp', 0.8738551139831543),\n",
       " ('rang', 0.873677670955658)]"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar([model.docvecs['positive']], topn=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# model.save(\"E:/dataset/MasterThesis/gensim_models/test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wordVectors = np.zeros((len(model.index2word), model.vector_size))\n",
    "for index, word in enumerate(model.index2word):\n",
    "    wordVectors[index,:] = model[word]\n",
    "\n",
    "sentimentVector = np.zeros((2, model.vector_size))\n",
    "sentimentVector[0,:] = model.docvecs['positive']\n",
    "sentimentVector[1,:] = model.docvecs['negative']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0. ,  0.1,  0.2,  0.3,  0.4,  0.5,  0.6,  0.7,  0.8,  0.9,  1. ])"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linspace(0,1,11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting iteration 2 of 100\n",
      "0.724\n",
      "Starting iteration 4 of 100\n",
      "0.707\n",
      "Starting iteration 6 of 100\n",
      "0.692666666667\n",
      "Starting iteration 8 of 100\n",
      "0.687333333333\n",
      "Starting iteration 10 of 100\n",
      "0.684666666667\n",
      "Starting iteration 12 of 100\n",
      "0.683333333333\n",
      "Starting iteration 14 of 100\n",
      "0.672\n",
      "Starting iteration 16 of 100\n",
      "0.666666666667\n",
      "Starting iteration 18 of 100\n",
      "0.662333333333\n",
      "Starting iteration 20 of 100\n",
      "0.657\n",
      "Starting iteration 22 of 100\n",
      "0.651333333333\n",
      "Starting iteration 24 of 100\n",
      "0.659\n",
      "Starting iteration 26 of 100\n",
      "0.666\n",
      "Starting iteration 28 of 100\n",
      "0.666\n",
      "Starting iteration 30 of 100\n",
      "0.664333333333\n",
      "Starting iteration 32 of 100\n",
      "0.669\n",
      "Starting iteration 34 of 100\n",
      "0.667\n",
      "Starting iteration 36 of 100\n",
      "0.668333333333\n",
      "Starting iteration 38 of 100\n",
      "0.676666666667\n",
      "Starting iteration 40 of 100\n",
      "0.672666666667\n",
      "Starting iteration 42 of 100\n",
      "0.674333333333\n",
      "Starting iteration 44 of 100\n",
      "0.681666666667\n",
      "Starting iteration 46 of 100\n",
      "0.678\n",
      "Starting iteration 48 of 100\n",
      "0.681\n",
      "Starting iteration 50 of 100\n",
      "0.679333333333\n",
      "Starting save model\n",
      "Starting iteration 52 of 100\n",
      "0.68\n",
      "Starting iteration 54 of 100\n",
      "0.683333333333\n",
      "Starting iteration 56 of 100\n",
      "0.679\n",
      "Starting iteration 58 of 100\n",
      "0.680333333333\n",
      "Starting iteration 60 of 100\n",
      "0.677\n",
      "Starting iteration 62 of 100\n",
      "0.676\n",
      "Starting iteration 64 of 100\n",
      "0.684333333333\n",
      "Starting iteration 66 of 100\n",
      "0.676333333333\n",
      "Starting iteration 68 of 100\n",
      "0.681333333333\n",
      "Starting iteration 70 of 100\n",
      "0.680666666667\n",
      "Starting iteration 72 of 100\n",
      "0.676333333333\n",
      "Starting iteration 74 of 100\n",
      "0.677666666667\n",
      "Starting iteration 76 of 100\n",
      "0.685333333333\n",
      "Starting iteration 78 of 100\n",
      "0.68\n",
      "Starting iteration 80 of 100\n",
      "0.679333333333\n",
      "Starting iteration 82 of 100\n",
      "0.677333333333\n",
      "Starting iteration 84 of 100\n",
      "0.679\n",
      "Starting iteration 86 of 100\n",
      "0.690666666667\n",
      "Starting iteration 88 of 100\n",
      "0.681666666667\n",
      "Starting iteration 90 of 100\n",
      "0.682\n",
      "Starting iteration 92 of 100\n",
      "0.683333333333\n",
      "Starting iteration 94 of 100\n",
      "0.688333333333\n",
      "Starting iteration 96 of 100\n",
      "0.688\n",
      "Starting iteration 98 of 100\n",
      "0.691666666667\n",
      "Starting iteration 100 of 100\n",
      "0.693\n",
      "Starting save model\n",
      "Starting iteration 2 of 100\n",
      "0.729666666667\n",
      "Starting iteration 4 of 100\n",
      "0.719\n",
      "Starting iteration 6 of 100\n",
      "0.704\n",
      "Starting iteration 8 of 100\n",
      "0.706\n",
      "Starting iteration 10 of 100\n",
      "0.701666666667\n",
      "Starting iteration 12 of 100\n",
      "0.709333333333\n",
      "Starting iteration 14 of 100\n",
      "0.702\n",
      "Starting iteration 16 of 100\n",
      "0.704666666667\n",
      "Starting iteration 18 of 100\n",
      "0.714\n",
      "Starting iteration 20 of 100\n",
      "0.711666666667\n",
      "Starting iteration 22 of 100\n",
      "0.718333333333\n",
      "Starting iteration 24 of 100\n",
      "0.726\n",
      "Starting iteration 26 of 100\n",
      "0.723666666667\n",
      "Starting iteration 28 of 100\n",
      "0.728333333333\n",
      "Starting iteration 30 of 100\n",
      "0.732333333333\n",
      "Starting iteration 32 of 100\n",
      "0.735333333333\n",
      "Starting iteration 34 of 100\n",
      "0.732\n",
      "Starting iteration 36 of 100\n",
      "0.740666666667\n",
      "Starting iteration 38 of 100\n",
      "0.733\n",
      "Starting iteration 40 of 100\n",
      "0.735666666667\n",
      "Starting iteration 42 of 100\n",
      "0.744\n",
      "Starting iteration 44 of 100\n",
      "0.739\n",
      "Starting iteration 46 of 100\n",
      "0.733333333333\n",
      "Starting iteration 48 of 100\n",
      "0.736\n",
      "Starting iteration 50 of 100\n",
      "0.733\n",
      "Starting save model\n",
      "Starting iteration 52 of 100\n",
      "0.729\n",
      "Starting iteration 54 of 100\n",
      "0.729333333333\n",
      "Starting iteration 56 of 100\n",
      "0.731666666667\n",
      "Starting iteration 58 of 100\n",
      "0.727333333333\n",
      "Starting iteration 60 of 100\n",
      "0.736333333333\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ratios = np.linspace(0,1,11)\n",
    "for ratio in ratios:\n",
    "    asum_embedding_path = \"E:/dataset/MasterThesis/Models/ASUM_embedding\" + str(ratio)\n",
    "    asum_embedding = ASUM_Embedding(wordVectors, sentimentVector, numTopics=10, alpha=0.01, beta=0.001, gamma=1, binary=ratio, numSentiments=2)\n",
    "    asum_embedding._initialize_(sentence_list_again, pos_neg_sentence_indices, pos_neg_sentiment_label, sentiment_label)\n",
    "    asum_embedding.run(sentence_list_again, save_path=asum_embedding_path, print_iter=2, save_iter = 50, maxIters= 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|████████████████▌                                                                  | 4/20 [01:18<05:15, 19.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start!! num_of_topic : 2, size : 100, window : 2, epoch : 5\n",
      "Starting iteration 10 of 100\n",
      "0.760142857143\n",
      "Starting iteration 20 of 100\n",
      "0.751428571429\n",
      "Starting save model\n",
      "Starting iteration 30 of 100\n",
      "0.749285714286\n",
      "Starting iteration 40 of 100\n",
      "0.747428571429\n",
      "Starting save model\n",
      "Starting iteration 50 of 100\n",
      "0.745285714286\n",
      "Starting iteration 60 of 100\n",
      "0.744571428571\n",
      "Starting save model\n",
      "Starting iteration 70 of 100\n",
      "0.743428571429\n",
      "Starting iteration 80 of 100\n",
      "0.741571428571\n",
      "Starting save model\n",
      "Starting iteration 90 of 100\n",
      "0.743571428571\n",
      "Starting iteration 100 of 100\n",
      "0.741285714286\n",
      "Starting save model\n",
      "END!! time : -1201.4870917797089\n",
      "start!! num_of_topic : 5, size : 100, window : 2, epoch : 5\n",
      "Starting iteration 10 of 100\n",
      "0.755142857143\n",
      "Starting iteration 20 of 100\n",
      "0.748\n",
      "Starting save model\n",
      "Starting iteration 30 of 100\n",
      "0.743571428571\n",
      "Starting iteration 40 of 100\n",
      "0.740285714286\n",
      "Starting save model\n",
      "Starting iteration 50 of 100\n",
      "0.732714285714\n",
      "Starting iteration 60 of 100\n",
      "0.733142857143\n",
      "Starting save model\n",
      "Starting iteration 70 of 100\n",
      "0.726714285714\n",
      "Starting iteration 80 of 100\n",
      "0.722\n",
      "Starting save model\n",
      "Starting iteration 90 of 100\n",
      "0.720857142857\n",
      "Starting iteration 100 of 100\n",
      "0.72\n",
      "Starting save model\n",
      "END!! time : -1794.4150087833405\n",
      "start!! num_of_topic : 10, size : 100, window : 2, epoch : 5\n",
      "Starting iteration 10 of 100\n",
      "0.750428571429\n",
      "Starting iteration 20 of 100\n",
      "0.743428571429\n",
      "Starting save model\n",
      "Starting iteration 30 of 100\n",
      "0.737571428571\n",
      "Starting iteration 40 of 100\n",
      "0.736285714286\n",
      "Starting save model\n",
      "Starting iteration 50 of 100\n",
      "0.735571428571\n",
      "Starting iteration 60 of 100\n",
      "0.737\n",
      "Starting save model\n",
      "Starting iteration 70 of 100\n",
      "0.731142857143\n",
      "Starting iteration 80 of 100\n",
      "0.734\n",
      "Starting save model\n",
      "Starting iteration 90 of 100\n",
      "0.732\n",
      "Starting iteration 100 of 100\n",
      "0.726857142857\n",
      "Starting save model\n",
      "END!! time : -2859.7906200885773\n",
      "start!! num_of_topic : 20, size : 100, window : 2, epoch : 5\n",
      "Starting iteration 10 of 100\n",
      "0.738857142857\n",
      "Starting iteration 20 of 100\n",
      "0.724714285714\n",
      "Starting save model\n",
      "Starting iteration 30 of 100\n",
      "0.721571428571\n",
      "Starting iteration 40 of 100\n",
      "0.721714285714\n",
      "Starting save model\n",
      "Starting iteration 50 of 100\n",
      "0.716571428571\n",
      "Starting iteration 60 of 100\n",
      "0.714714285714\n",
      "Starting save model\n",
      "Starting iteration 70 of 100\n",
      "0.711571428571\n",
      "Starting iteration 80 of 100\n",
      "0.714142857143\n",
      "Starting save model\n",
      "Starting iteration 90 of 100\n",
      "0.712428571429\n",
      "Starting iteration 100 of 100\n",
      "0.708714285714\n",
      "Starting save model\n",
      "END!! time : -4902.647294282913\n",
      "start!! num_of_topic : 50, size : 100, window : 2, epoch : 5\n",
      "Starting iteration 10 of 100\n",
      "0.739714285714\n",
      "Starting iteration 20 of 100\n",
      "0.735428571429\n",
      "Starting save model\n",
      "Starting iteration 30 of 100\n",
      "0.738714285714\n",
      "Starting iteration 40 of 100\n",
      "0.733571428571\n",
      "Starting save model\n",
      "Starting iteration 50 of 100\n",
      "0.730714285714\n",
      "Starting iteration 60 of 100\n",
      "0.729285714286\n",
      "Starting save model\n",
      "Starting iteration 70 of 100\n",
      "0.722857142857\n",
      "Starting iteration 80 of 100\n",
      "0.723142857143\n",
      "Starting save model\n",
      "Starting iteration 90 of 100\n",
      "0.723428571429\n",
      "Starting iteration 100 of 100\n",
      "0.719714285714\n",
      "Starting save model\n",
      "END!! time : -10845.364221811295\n",
      "start!! num_of_topic : 100, size : 100, window : 2, epoch : 5\n",
      "Starting iteration 10 of 100\n",
      "0.723142857143\n",
      "Starting iteration 20 of 100\n",
      "0.711714285714\n",
      "Starting save model\n",
      "Starting iteration 30 of 100\n",
      "0.702428571429\n",
      "Starting iteration 40 of 100\n",
      "0.697142857143\n",
      "Starting save model\n",
      "Starting iteration 50 of 100\n",
      "0.694857142857\n",
      "Starting iteration 60 of 100\n",
      "0.696714285714\n",
      "Starting save model\n",
      "Starting iteration 70 of 100\n",
      "0.694285714286\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-123-5c74837375e8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'time'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'#parmeter search\\nmodel_path = \"E:/dataset/MasterThesis/gensim_models/\"\\nasum_embedding_path = \"E:/dataset/MasterThesis/Models/ASUM_embedding\"\\n\\nwindow = [2,3,4,5]\\nsize = [100,200,300]\\nnum_topics = [2,5,10,20,50,100]\\npasses = 20\\nfor w in window:\\n    for s in size:\\n        model = Doc2Vec(dm=1, dm_mean=1, negative =10,min_count=0, sample=1e-5,\\n                        window=w, size=s, \\n                        workers=multiprocessing.cpu_count(), \\n                        alpha=0.025, min_alpha=0.025)\\n        model.build_vocab(documents)\\n\\n        for epoch in tqdm(range(passes)):\\n            random.shuffle(documents)\\n            model.train(documents)\\n            model.alpha -= 0.002  # decrease the learning rate\\n            model.min_alpha = model.alpha  # fix the learning rate, no decay\\n            if (epoch + 1) % 5 ==0:\\n                model.save(model_path + \\'model_\\' + str(w) + \\'_\\' + str(s) + \\'_\\' + str(epoch+1))\\n                \\n                # asum embedding model\\n                wordVectors = np.zeros((len(model.index2word), model.vector_size))\\n                for index, word in enumerate(model.index2word):\\n                    wordVectors[index,:] = model[word]\\n\\n                sentimentVector = np.zeros((2, model.vector_size))\\n                sentimentVector[0,:] = model.docvecs[\\'positive\\']\\n                sentimentVector[1,:] = model.docvecs[\\'negative\\']\\n                for k in num_topics:\\n                    print(\"start!! num_of_topic : %s, size : %s, window : %s, epoch : %s\"%(k,s,w,epoch+1))\\n                    start = time.time()\\n                    asum_embedding = ASUM_Embedding(wordVectors, sentimentVector, numTopics=k, alpha=0.01, beta=0.001, gamma=1, numSentiments=2)\\n                    asum_embedding._initialize_(sentence_list_again, pos_neg_sentence_indices, pos_neg_sentiment_label, sentiment_label)\\n                    asum_embedding.run(sentence_list_again, save_path=asum_embedding_path, print_iter=10, save_iter = 20, maxIters= 100)\\n                    end = time.time()\\n                    print(\"END!! time :\", start-end)'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32mC:\\Program Files\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2113\u001b[0m             \u001b[0mmagic_arg_s\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstack_depth\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   2114\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2115\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2116\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   2117\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<decorator-gen-60>\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n",
      "\u001b[0;32mC:\\Program Files\\Anaconda3\\lib\\site-packages\\IPython\\core\\magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[1;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Program Files\\Anaconda3\\lib\\site-packages\\IPython\\core\\magics\\execution.py\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[1;32m   1178\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m             \u001b[0mst\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1180\u001b[0;31m             \u001b[0mexec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mglob\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlocal_ns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1181\u001b[0m             \u001b[0mend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mD:\\Dropbox\\2016-2\\졸업논문\\STMD\\ASUM_Embedding.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, reviews, save_path, print_iter, save_iter, maxIters)\u001b[0m\n\u001b[1;32m    300\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumDocs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0mm\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumSentence\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 302\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msampling\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mm\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    303\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mD:\\Dropbox\\2016-2\\졸업논문\\STMD\\ASUM_Embedding.py\u001b[0m in \u001b[0;36msampling\u001b[0;34m(self, d, m)\u001b[0m\n\u001b[1;32m    152\u001b[0m         topic_similarity = ot.softmax(np.dot(self.topicVectors,\n\u001b[1;32m    153\u001b[0m                                              self.wordVectors[\n\u001b[0;32m--> 154\u001b[0;31m                                                  self.doc_sent_word_dict[d][m]].T))  # ( K x num words in sentence)\n\u001b[0m\u001b[1;32m    155\u001b[0m         senti_similarity = ot.softmax(np.dot(self.sentimentVector,\n\u001b[1;32m    156\u001b[0m                                              self.wordVectors[\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#parmeter search\n",
    "model_path = \"E:/dataset/MasterThesis/gensim_models/\"\n",
    "asum_embedding_path = \"E:/dataset/MasterThesis/Models/ASUM_embedding\"\n",
    "\n",
    "window = [2,3,4,5]\n",
    "size = [100,200,300]\n",
    "num_topics = [2,5,10,20,50,100]\n",
    "passes = 20\n",
    "for w in window:\n",
    "    for s in size:\n",
    "        model = Doc2Vec(dm=1, dm_mean=1, negative =10,min_count=0, sample=1e-5,\n",
    "                        window=w, size=s, \n",
    "                        workers=multiprocessing.cpu_count(), \n",
    "                        alpha=0.025, min_alpha=0.025)\n",
    "        model.build_vocab(documents)\n",
    "\n",
    "        for epoch in tqdm(range(passes)):\n",
    "            random.shuffle(documents)\n",
    "            model.train(documents)\n",
    "            model.alpha -= 0.002  # decrease the learning rate\n",
    "            model.min_alpha = model.alpha  # fix the learning rate, no decay\n",
    "            if (epoch + 1) % 5 ==0:\n",
    "                model.save(model_path + 'model_' + str(w) + '_' + str(s) + '_' + str(epoch+1))\n",
    "                \n",
    "                # asum embedding model\n",
    "                wordVectors = np.zeros((len(model.index2word), model.vector_size))\n",
    "                for index, word in enumerate(model.index2word):\n",
    "                    wordVectors[index,:] = model[word]\n",
    "\n",
    "                sentimentVector = np.zeros((2, model.vector_size))\n",
    "                sentimentVector[0,:] = model.docvecs['positive']\n",
    "                sentimentVector[1,:] = model.docvecs['negative']\n",
    "                for k in num_topics:\n",
    "                    print(\"start!! num_of_topic : %s, size : %s, window : %s, epoch : %s\"%(k,s,w,epoch+1))\n",
    "                    start = time.time()\n",
    "                    asum_embedding = ASUM_Embedding(wordVectors, sentimentVector, numTopics=k, alpha=0.01, beta=0.001, gamma=1, numSentiments=2)\n",
    "                    asum_embedding._initialize_(sentence_list_again, pos_neg_sentence_indices, pos_neg_sentiment_label, sentiment_label)\n",
    "                    asum_embedding.run(sentence_list_again, save_path=asum_embedding_path, print_iter=10, save_iter = 20, maxIters= 100)\n",
    "                    end = time.time()\n",
    "                    print(\"END!! time :\", start-end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|████████████████▌                                                                  | 4/20 [01:19<05:14, 19.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start!! num_of_topic : 50, size : 100, window : 3, epoch : 5\n",
      "Starting iteration 10 of 100\n",
      "0.742428571429\n",
      "Starting iteration 20 of 100\n",
      "0.741857142857\n",
      "Starting save model\n",
      "Starting iteration 30 of 100\n",
      "0.744142857143\n",
      "Starting iteration 40 of 100\n",
      "0.738857142857\n",
      "Starting save model\n",
      "Starting iteration 50 of 100\n",
      "0.735\n",
      "Starting iteration 60 of 100\n",
      "0.728428571429\n",
      "Starting save model\n",
      "Starting iteration 70 of 100\n",
      "0.725714285714\n",
      "Starting iteration 80 of 100\n",
      "0.721142857143\n",
      "Starting save model\n",
      "Starting iteration 90 of 100\n",
      "0.720142857143\n",
      "Starting iteration 100 of 100\n",
      "0.715857142857\n",
      "Starting save model\n",
      "END!! time : -11032.702196121216\n",
      "start!! num_of_topic : 30, size : 100, window : 3, epoch : 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-124-00a8f5338e94>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'time'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'#parmeter search\\nmodel_path = \"E:/dataset/MasterThesis/gensim_models/\"\\nasum_embedding_path = \"E:/dataset/MasterThesis/Models/ASUM_embedding\"\\n\\nwindow = [3,4,5]\\nsize = [100,200,300]\\nnum_topics = [50,30,20,10,5,2]\\npasses = 20\\nfor w in window:\\n    for s in size:\\n        model = Doc2Vec(dm=1, dm_mean=1, negative =10,min_count=0, sample=1e-5,\\n                        window=w, size=s, \\n                        workers=multiprocessing.cpu_count(), \\n                        alpha=0.025, min_alpha=0.025)\\n        model.build_vocab(documents)\\n\\n        for epoch in tqdm(range(passes)):\\n            random.shuffle(documents)\\n            model.train(documents)\\n            model.alpha -= 0.002  # decrease the learning rate\\n            model.min_alpha = model.alpha  # fix the learning rate, no decay\\n            if (epoch + 1) % 5 ==0:\\n                model.save(model_path + \\'model_\\' + str(w) + \\'_\\' + str(s) + \\'_\\' + str(epoch+1))\\n                \\n                # asum embedding model\\n                wordVectors = np.zeros((len(model.index2word), model.vector_size))\\n                for index, word in enumerate(model.index2word):\\n                    wordVectors[index,:] = model[word]\\n\\n                sentimentVector = np.zeros((2, model.vector_size))\\n                sentimentVector[0,:] = model.docvecs[\\'positive\\']\\n                sentimentVector[1,:] = model.docvecs[\\'negative\\']\\n                for k in num_topics:\\n                    print(\"start!! num_of_topic : %s, size : %s, window : %s, epoch : %s\"%(k,s,w,epoch+1))\\n                    start = time.time()\\n                    asum_embedding = ASUM_Embedding(wordVectors, sentimentVector, numTopics=k, alpha=0.01, beta=0.001, gamma=1, numSentiments=2)\\n                    asum_embedding._initialize_(sentence_list_again, pos_neg_sentence_indices, pos_neg_sentiment_label, sentiment_label)\\n                    asum_embedding.run(sentence_list_again, save_path=asum_embedding_path, print_iter=10, save_iter = 20, maxIters= 100)\\n                    end = time.time()\\n                    print(\"END!! time :\", start-end)'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32mC:\\Program Files\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2113\u001b[0m             \u001b[0mmagic_arg_s\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstack_depth\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   2114\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2115\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2116\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   2117\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<decorator-gen-60>\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n",
      "\u001b[0;32mC:\\Program Files\\Anaconda3\\lib\\site-packages\\IPython\\core\\magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[1;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Program Files\\Anaconda3\\lib\\site-packages\\IPython\\core\\magics\\execution.py\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[1;32m   1178\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m             \u001b[0mst\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1180\u001b[0;31m             \u001b[0mexec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mglob\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlocal_ns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1181\u001b[0m             \u001b[0mend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mD:\\Dropbox\\2016-2\\졸업논문\\STMD\\ASUM_Embedding.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, reviews, save_path, print_iter, save_iter, maxIters)\u001b[0m\n\u001b[1;32m    300\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumDocs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0mm\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumSentence\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 302\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msampling\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mm\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    303\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mD:\\Dropbox\\2016-2\\졸업논문\\STMD\\ASUM_Embedding.py\u001b[0m in \u001b[0;36msampling\u001b[0;34m(self, d, m)\u001b[0m\n\u001b[1;32m    152\u001b[0m         topic_similarity = ot.softmax(np.dot(self.topicVectors,\n\u001b[1;32m    153\u001b[0m                                              self.wordVectors[\n\u001b[0;32m--> 154\u001b[0;31m                                                  self.doc_sent_word_dict[d][m]].T))  # ( K x num words in sentence)\n\u001b[0m\u001b[1;32m    155\u001b[0m         senti_similarity = ot.softmax(np.dot(self.sentimentVector,\n\u001b[1;32m    156\u001b[0m                                              self.wordVectors[\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#parmeter search\n",
    "model_path = \"E:/dataset/MasterThesis/gensim_models/\"\n",
    "asum_embedding_path = \"E:/dataset/MasterThesis/Models/ASUM_embedding\"\n",
    "\n",
    "window = [3,4,5]\n",
    "size = [100,200,300]\n",
    "num_topics = [50,30,20,10,5,2]\n",
    "passes = 20\n",
    "for w in window:\n",
    "    for s in size:\n",
    "        model = Doc2Vec(dm=1, dm_mean=1, negative =10,min_count=0, sample=1e-5,\n",
    "                        window=w, size=s, \n",
    "                        workers=multiprocessing.cpu_count(), \n",
    "                        alpha=0.025, min_alpha=0.025)\n",
    "        model.build_vocab(documents)\n",
    "\n",
    "        for epoch in tqdm(range(passes)):\n",
    "            random.shuffle(documents)\n",
    "            model.train(documents)\n",
    "            model.alpha -= 0.002  # decrease the learning rate\n",
    "            model.min_alpha = model.alpha  # fix the learning rate, no decay\n",
    "            if (epoch + 1) % 5 ==0:\n",
    "                model.save(model_path + 'model_' + str(w) + '_' + str(s) + '_' + str(epoch+1))\n",
    "                \n",
    "                # asum embedding model\n",
    "                wordVectors = np.zeros((len(model.index2word), model.vector_size))\n",
    "                for index, word in enumerate(model.index2word):\n",
    "                    wordVectors[index,:] = model[word]\n",
    "\n",
    "                sentimentVector = np.zeros((2, model.vector_size))\n",
    "                sentimentVector[0,:] = model.docvecs['positive']\n",
    "                sentimentVector[1,:] = model.docvecs['negative']\n",
    "                for k in num_topics:\n",
    "                    print(\"start!! num_of_topic : %s, size : %s, window : %s, epoch : %s\"%(k,s,w,epoch+1))\n",
    "                    start = time.time()\n",
    "                    asum_embedding = ASUM_Embedding(wordVectors, sentimentVector, numTopics=k, alpha=0.01, beta=0.001, gamma=1, numSentiments=2)\n",
    "                    asum_embedding._initialize_(sentence_list_again, pos_neg_sentence_indices, pos_neg_sentiment_label, sentiment_label)\n",
    "                    asum_embedding.run(sentence_list_again, save_path=asum_embedding_path, print_iter=10, save_iter = 20, maxIters= 100)\n",
    "                    end = time.time()\n",
    "                    print(\"END!! time :\", end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic1</th>\n",
       "      <th>Topic10</th>\n",
       "      <th>Topic11</th>\n",
       "      <th>Topic12</th>\n",
       "      <th>Topic13</th>\n",
       "      <th>Topic14</th>\n",
       "      <th>Topic15</th>\n",
       "      <th>Topic16</th>\n",
       "      <th>Topic17</th>\n",
       "      <th>Topic18</th>\n",
       "      <th>...</th>\n",
       "      <th>Topic28</th>\n",
       "      <th>Topic29</th>\n",
       "      <th>Topic3</th>\n",
       "      <th>Topic30</th>\n",
       "      <th>Topic4</th>\n",
       "      <th>Topic5</th>\n",
       "      <th>Topic6</th>\n",
       "      <th>Topic7</th>\n",
       "      <th>Topic8</th>\n",
       "      <th>Topic9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>camera</td>\n",
       "      <td>camera</td>\n",
       "      <td>tv</td>\n",
       "      <td>camera</td>\n",
       "      <td>use</td>\n",
       "      <td>use</td>\n",
       "      <td>tv</td>\n",
       "      <td>use</td>\n",
       "      <td>camera</td>\n",
       "      <td>camera</td>\n",
       "      <td>...</td>\n",
       "      <td>one</td>\n",
       "      <td>batteri</td>\n",
       "      <td>camera</td>\n",
       "      <td>use</td>\n",
       "      <td>camera</td>\n",
       "      <td>use</td>\n",
       "      <td>use</td>\n",
       "      <td>use</td>\n",
       "      <td>use</td>\n",
       "      <td>camera</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>use</td>\n",
       "      <td>len</td>\n",
       "      <td>set</td>\n",
       "      <td>great</td>\n",
       "      <td>samsung</td>\n",
       "      <td>work</td>\n",
       "      <td>samsung</td>\n",
       "      <td>samsung</td>\n",
       "      <td>bought</td>\n",
       "      <td>one</td>\n",
       "      <td>...</td>\n",
       "      <td>samsung</td>\n",
       "      <td>camera</td>\n",
       "      <td>use</td>\n",
       "      <td>camera</td>\n",
       "      <td>one</td>\n",
       "      <td>return</td>\n",
       "      <td>time</td>\n",
       "      <td>samsung</td>\n",
       "      <td>one</td>\n",
       "      <td>canon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>one</td>\n",
       "      <td>use</td>\n",
       "      <td>use</td>\n",
       "      <td>canon</td>\n",
       "      <td>one</td>\n",
       "      <td>one</td>\n",
       "      <td>one</td>\n",
       "      <td>one</td>\n",
       "      <td>one</td>\n",
       "      <td>use</td>\n",
       "      <td>...</td>\n",
       "      <td>appl</td>\n",
       "      <td>use</td>\n",
       "      <td>get</td>\n",
       "      <td>one</td>\n",
       "      <td>use</td>\n",
       "      <td>camera</td>\n",
       "      <td>get</td>\n",
       "      <td>problem</td>\n",
       "      <td>product</td>\n",
       "      <td>len</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>get</td>\n",
       "      <td>one</td>\n",
       "      <td>get</td>\n",
       "      <td>pictur</td>\n",
       "      <td>appl</td>\n",
       "      <td>time</td>\n",
       "      <td>use</td>\n",
       "      <td>tablet</td>\n",
       "      <td>use</td>\n",
       "      <td>purchas</td>\n",
       "      <td>...</td>\n",
       "      <td>get</td>\n",
       "      <td>tablet</td>\n",
       "      <td>tv</td>\n",
       "      <td>product</td>\n",
       "      <td>appl</td>\n",
       "      <td>one</td>\n",
       "      <td>hour</td>\n",
       "      <td>camera</td>\n",
       "      <td>would</td>\n",
       "      <td>would</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>tablet</td>\n",
       "      <td>get</td>\n",
       "      <td>would</td>\n",
       "      <td>love</td>\n",
       "      <td>get</td>\n",
       "      <td>veri</td>\n",
       "      <td>would</td>\n",
       "      <td>return</td>\n",
       "      <td>screen</td>\n",
       "      <td>product</td>\n",
       "      <td>...</td>\n",
       "      <td>drive</td>\n",
       "      <td>charg</td>\n",
       "      <td>samsung</td>\n",
       "      <td>would</td>\n",
       "      <td>product</td>\n",
       "      <td>amazon</td>\n",
       "      <td>tri</td>\n",
       "      <td>updat</td>\n",
       "      <td>appl</td>\n",
       "      <td>samsung</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>would</td>\n",
       "      <td>would</td>\n",
       "      <td>samsung</td>\n",
       "      <td>use</td>\n",
       "      <td>would</td>\n",
       "      <td>would</td>\n",
       "      <td>product</td>\n",
       "      <td>amazon</td>\n",
       "      <td>veri</td>\n",
       "      <td>get</td>\n",
       "      <td>...</td>\n",
       "      <td>use</td>\n",
       "      <td>video</td>\n",
       "      <td>one</td>\n",
       "      <td>like</td>\n",
       "      <td>get</td>\n",
       "      <td>work</td>\n",
       "      <td>batteri</td>\n",
       "      <td>issu</td>\n",
       "      <td>get</td>\n",
       "      <td>one</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>need</td>\n",
       "      <td>canon</td>\n",
       "      <td>pictur</td>\n",
       "      <td>bought</td>\n",
       "      <td>tv</td>\n",
       "      <td>tv</td>\n",
       "      <td>get</td>\n",
       "      <td>tv</td>\n",
       "      <td>great</td>\n",
       "      <td>would</td>\n",
       "      <td>...</td>\n",
       "      <td>would</td>\n",
       "      <td>gb</td>\n",
       "      <td>len</td>\n",
       "      <td>canon</td>\n",
       "      <td>buy</td>\n",
       "      <td>get</td>\n",
       "      <td>tablet</td>\n",
       "      <td>work</td>\n",
       "      <td>devic</td>\n",
       "      <td>get</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>video</td>\n",
       "      <td>veri</td>\n",
       "      <td>screen</td>\n",
       "      <td>len</td>\n",
       "      <td>amazon</td>\n",
       "      <td>like</td>\n",
       "      <td>work</td>\n",
       "      <td>camera</td>\n",
       "      <td>new</td>\n",
       "      <td>samsung</td>\n",
       "      <td>...</td>\n",
       "      <td>tv</td>\n",
       "      <td>turn</td>\n",
       "      <td>take</td>\n",
       "      <td>thing</td>\n",
       "      <td>purchas</td>\n",
       "      <td>buy</td>\n",
       "      <td>would</td>\n",
       "      <td>tablet</td>\n",
       "      <td>like</td>\n",
       "      <td>repair</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>work</td>\n",
       "      <td>tv</td>\n",
       "      <td>turn</td>\n",
       "      <td>one</td>\n",
       "      <td>like</td>\n",
       "      <td>app</td>\n",
       "      <td>purchas</td>\n",
       "      <td>product</td>\n",
       "      <td>love</td>\n",
       "      <td>tv</td>\n",
       "      <td>...</td>\n",
       "      <td>amazon</td>\n",
       "      <td>flash</td>\n",
       "      <td>pictur</td>\n",
       "      <td>veri</td>\n",
       "      <td>bought</td>\n",
       "      <td>love</td>\n",
       "      <td>one</td>\n",
       "      <td>one</td>\n",
       "      <td>tablet</td>\n",
       "      <td>use</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>also</td>\n",
       "      <td>time</td>\n",
       "      <td>sound</td>\n",
       "      <td>return</td>\n",
       "      <td>work</td>\n",
       "      <td>problem</td>\n",
       "      <td>set</td>\n",
       "      <td>would</td>\n",
       "      <td>ipad</td>\n",
       "      <td>new</td>\n",
       "      <td>...</td>\n",
       "      <td>new</td>\n",
       "      <td>ppi</td>\n",
       "      <td>would</td>\n",
       "      <td>work</td>\n",
       "      <td>canon</td>\n",
       "      <td>canon</td>\n",
       "      <td>work</td>\n",
       "      <td>app</td>\n",
       "      <td>comput</td>\n",
       "      <td>amazon</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Topic1 Topic10  Topic11 Topic12  Topic13  Topic14  Topic15  Topic16  \\\n",
       "0  camera  camera       tv  camera      use      use       tv      use   \n",
       "1     use     len      set   great  samsung     work  samsung  samsung   \n",
       "2     one     use      use   canon      one      one      one      one   \n",
       "3     get     one      get  pictur     appl     time      use   tablet   \n",
       "4  tablet     get    would    love      get     veri    would   return   \n",
       "5   would   would  samsung     use    would    would  product   amazon   \n",
       "6    need   canon   pictur  bought       tv       tv      get       tv   \n",
       "7   video    veri   screen     len   amazon     like     work   camera   \n",
       "8    work      tv     turn     one     like      app  purchas  product   \n",
       "9    also    time    sound  return     work  problem      set    would   \n",
       "\n",
       "  Topic17  Topic18   ...     Topic28  Topic29   Topic3  Topic30   Topic4  \\\n",
       "0  camera   camera   ...         one  batteri   camera      use   camera   \n",
       "1  bought      one   ...     samsung   camera      use   camera      one   \n",
       "2     one      use   ...        appl      use      get      one      use   \n",
       "3     use  purchas   ...         get   tablet       tv  product     appl   \n",
       "4  screen  product   ...       drive    charg  samsung    would  product   \n",
       "5    veri      get   ...         use    video      one     like      get   \n",
       "6   great    would   ...       would       gb      len    canon      buy   \n",
       "7     new  samsung   ...          tv     turn     take    thing  purchas   \n",
       "8    love       tv   ...      amazon    flash   pictur     veri   bought   \n",
       "9    ipad      new   ...         new      ppi    would     work    canon   \n",
       "\n",
       "   Topic5   Topic6   Topic7   Topic8   Topic9  \n",
       "0     use      use      use      use   camera  \n",
       "1  return     time  samsung      one    canon  \n",
       "2  camera      get  problem  product      len  \n",
       "3     one     hour   camera    would    would  \n",
       "4  amazon      tri    updat     appl  samsung  \n",
       "5    work  batteri     issu      get      one  \n",
       "6     get   tablet     work    devic      get  \n",
       "7     buy    would   tablet     like   repair  \n",
       "8    love      one      one   tablet      use  \n",
       "9   canon     work      app   comput   amazon  \n",
       "\n",
       "[10 rows x 30 columns]"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "asum_embedding.getTopKWordsByTopic(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12.6234"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documents) / 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
