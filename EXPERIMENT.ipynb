{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from ast import literal_eval\n",
    "from collections import Counter\n",
    "from scipy.optimize import fmin_l_bfgs_b\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "from gensim.models import Doc2Vec\n",
    "import multiprocessing\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import optimizeTopicVectors as ot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# hoseong's class\n",
    "from STMD import *\n",
    "from ASUM import *\n",
    "from ASUM_Embedding import *\n",
    "from preprocess import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load data\n",
    "# work_path = \"/media/hs-ubuntu/data/dataset/MasterThesis/STMD_data/\"\n",
    "work_path = \"E:/dataset/MasterThesis/STMD_data/\"\n",
    "\n",
    "data = pd.read_csv(work_path + \"preprocess_complete_Electronics.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "brand = ['Apple', 'Samsung','Canon']\n",
    "brand_df = data[data.brand.isin(brand)]\n",
    "brand_df.reset_index(drop=True, inplace=True)\n",
    "#긍정, 부정 반반씩\n",
    "pos_reviews = brand_df[brand_df.overall >= 4]\n",
    "neg_reviews = brand_df[brand_df.overall <= 2]\n",
    "pos_sample = pos_reviews.sample(3500, random_state=23)\n",
    "neg_sample = neg_reviews.sample(3500, random_state=42)\n",
    "df = pd.concat([pos_sample, neg_sample], axis=0)\n",
    "df['preprocessed'] = df.preprocessed.apply(lambda row: literal_eval(row))\n",
    "df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3500, 12)\n",
      "(3500, 12)\n"
     ]
    }
   ],
   "source": [
    "# 긍/부정 ratio\n",
    "print(pos_sample.shape)\n",
    "print(neg_sample.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# prepare\n",
    "sentence_list, sentiment_label, sentence_senti_label, \\\n",
    "pos_neg_sentence_indices, pos_neg_sentiment_label, numSentence = prepare(df)\n",
    "\n",
    "documents, sentence_list_again, bigram, documents_label\\\n",
    "= bigram_and_sentence(sentence_senti_label, sentence_list, numSentence, max_vocab=5000, threshold = 5, min_count = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TaggedDocument(words=['ye', 'know', 'camera', 'crop_sensor', 'pleas', 'prime', 'lower', 'end', 'take', 'better', 'qualiti', 'photo', 'thing'], tags=['negative'])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[7000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 20/20 [06:14<00:00, 19.41s/it]\n"
     ]
    }
   ],
   "source": [
    "# word / sentiment embedding\n",
    "\n",
    "window = [2]\n",
    "size = [100]\n",
    "passes = 20\n",
    "for w in window:\n",
    "    for s in size:\n",
    "        model = Doc2Vec(dm=1, \n",
    "                        dm_mean=1, \n",
    "                        negative =10,\n",
    "                        min_count=0, sample=1e-5,\n",
    "                        window=w, size=s, \n",
    "                        workers=multiprocessing.cpu_count(), \n",
    "                        alpha=0.025, min_alpha=0.025)\n",
    "        model.build_vocab(documents)\n",
    "\n",
    "        for epoch in tqdm(range(passes)):\n",
    "            random.shuffle(documents)\n",
    "            model.train(documents)\n",
    "            model.alpha -= 0.002  # decrease the learning rate\n",
    "            model.min_alpha = model.alpha  # fix the learning rate, no decay\n",
    "            if (epoch + 1) % 5 ==0:\n",
    "                model.save(model_path + 'model_' + str(w) + '_' + str(s) + '_' + str(epoch+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = Doc2Vec.load(model_path + 'model_2_100_10')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wordVectors = np.zeros((len(model.index2word), model.vector_size))\n",
    "for index, word in enumerate(model.index2word):\n",
    "    wordVectors[index,:] = model[word]\n",
    "\n",
    "sentimentVector = np.zeros((2, model.vector_size))\n",
    "sentimentVector[0,:] = model.docvecs['positive']\n",
    "sentimentVector[1,:] = model.docvecs['negative']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting iteration 2 of 3\n",
      "0.792\n",
      "Starting save model\n",
      "Wall time: 1min 32s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "asum_embedding = ASUM_Embedding(wordVectors, sentimentVector, numTopics=10, alpha=0.01, beta=0.001, gamma=1, numSentiments=2)\n",
    "asum_embedding._initialize_(sentence_list_again, pos_neg_sentence_indices, pos_neg_sentiment_label, sentiment_label)\n",
    "asum_embedding.run(sentence_list_again, save_path=asum_embedding_path, print_iter=2, save_iter = 3, maxIters= 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting iteration 2 of 10\n",
      "0.585428571429\n",
      "Starting iteration 4 of 10\n",
      "0.615428571429\n",
      "Starting iteration 6 of 10\n",
      "0.614571428571\n",
      "Starting iteration 8 of 10\n",
      "0.605714285714\n",
      "Starting iteration 10 of 10\n",
      "0.602428571429\n"
     ]
    }
   ],
   "source": [
    "stmd = STMD(wordVectors, sentimentVector, numTopics=10, alpha=0.01, beta=0.001, gamma=1, numSentiments=2)\n",
    "stmd._initialize_(sentence_list_again, pos_neg_sentence_indices, pos_neg_sentiment_label, sentiment_label)\n",
    "stmd.run(sentence_list_again, print_iter=2, maxIters= 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting iteration 2 of 3\n",
      "0.789\n",
      "Starting save model\n",
      "Wall time: 1min 13s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "asum_path = \"E:/dataset/MasterThesis/Models/ASUM_\"\n",
    "asum = ASUM(wordVectors, sentimentVector, numTopics=10, alpha=0.01, beta=0.001, gamma=1, numSentiments=2)\n",
    "asum._initialize_(sentence_list_again, pos_neg_sentence_indices, pos_neg_sentiment_label, sentiment_label)\n",
    "asum.run(sentence_list_again, save_path=asum_path, print_iter=2, save_iter = 3, maxIters= 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting iteration 2 of 3\n",
      "0.792285714286\n",
      "Starting save model\n",
      "Wall time: 1min 29s\n"
     ]
    }
   ],
   "source": [
    "%%time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|████████████████▌                                                                  | 4/20 [01:20<05:19, 19.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size : 100, window : 2, epoch : 5, completed!!!\n",
      "--- topic modeling start, num of topic : 2----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-117-3cba64f1c575>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     36\u001b[0m                     \u001b[0masum_embedding\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mASUM_Embedding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwordVectors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msentimentVector\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnumTopics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.01\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbeta\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.001\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnumSentiments\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m                     \u001b[0masum_embedding\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_initialize_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentence_list_again\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpos_neg_sentence_indices\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpos_neg_sentiment_label\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msentiment_label\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m                     \u001b[0masum_embedding\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentence_list_again\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msave_path\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0masum_embedding_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprint_iter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msave_iter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmaxIters\u001b[0m\u001b[1;33m=\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32mD:\\Dropbox\\2016-2\\졸업논문\\STMD\\ASUM_Embedding.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, reviews, save_path, print_iter, save_iter, maxIters)\u001b[0m\n\u001b[1;32m    300\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumDocs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0mm\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumSentence\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 302\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msampling\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mm\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    303\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mD:\\Dropbox\\2016-2\\졸업논문\\STMD\\ASUM_Embedding.py\u001b[0m in \u001b[0;36msampling\u001b[0;34m(self, d, m)\u001b[0m\n\u001b[1;32m    152\u001b[0m         topic_similarity = ot.softmax(np.dot(self.topicVectors,\n\u001b[1;32m    153\u001b[0m                                              self.wordVectors[\n\u001b[0;32m--> 154\u001b[0;31m                                                  self.doc_sent_word_dict[d][m]].T))  # ( K x num words in sentence)\n\u001b[0m\u001b[1;32m    155\u001b[0m         senti_similarity = ot.softmax(np.dot(self.sentimentVector,\n\u001b[1;32m    156\u001b[0m                                              self.wordVectors[\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#parmeter search\n",
    "model_path = \"E:/dataset/MasterThesis/gensim_models/\"\n",
    "asum_embedding_path = \"E:/dataset/MasterThesis/Models/ASUM_embedding\"\n",
    "\n",
    "window = [2,3,5]\n",
    "size = [100,200,300]\n",
    "num_topics = [2,5,10,20,50,100]\n",
    "passes = 20\n",
    "for w in window:\n",
    "    for s in size:\n",
    "        model = Doc2Vec(dm=1, dm_mean=1, negative =10,min_count=0, sample=1e-5,\n",
    "                        window=w, size=s, \n",
    "                        workers=multiprocessing.cpu_count(), \n",
    "                        alpha=0.025, min_alpha=0.025)\n",
    "        model.build_vocab(documents)\n",
    "\n",
    "        for epoch in tqdm(range(passes)):\n",
    "            random.shuffle(documents)\n",
    "            model.train(documents)\n",
    "            model.alpha -= 0.002  # decrease the learning rate\n",
    "            model.min_alpha = model.alpha  # fix the learning rate, no decay\n",
    "            if (epoch + 1) % 5 ==0:\n",
    "                model.save(model_path + 'model_' + str(w) + '_' + str(s) + '_' + str(epoch+1))\n",
    "                \n",
    "                # asum embedding model\n",
    "                wordVectors = np.zeros((len(model.index2word), model.vector_size))\n",
    "                for index, word in enumerate(model.index2word):\n",
    "                    wordVectors[index,:] = model[word]\n",
    "\n",
    "                sentimentVector = np.zeros((2, model.vector_size))\n",
    "                sentimentVector[0,:] = model.docvecs['positive']\n",
    "                sentimentVector[1,:] = model.docvecs['negative']\n",
    "                for k in num_topics:\n",
    "                    print(\"num_of_topic : %s, size : %s, window : %s, epoch : %s, completed!!!\"%(k,s,w,epoch+1))\n",
    "                    asum_embedding = ASUM_Embedding(wordVectors, sentimentVector, numTopics=k, alpha=0.01, beta=0.001, gamma=1, numSentiments=2)\n",
    "                    asum_embedding._initialize_(sentence_list_again, pos_neg_sentence_indices, pos_neg_sentiment_label, sentiment_label)\n",
    "                    asum_embedding.run(sentence_list_again, save_path=asum_embedding_path, print_iter=10, save_iter = 20, maxIters= 100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
