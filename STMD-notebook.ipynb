{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from ast import literal_eval\n",
    "from collections import Counter\n",
    "from scipy.optimize import fmin_l_bfgs_b\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "from gensim.models import Doc2Vec\n",
    "import multiprocessing\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import optimizeTopicVectors as ot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sampleFromDirichlet(alpha):\n",
    "    return np.random.dirichlet(alpha)\n",
    "\n",
    "\n",
    "def sampleFromCategorical(theta):\n",
    "    # theta = theta / np.sum(theta)\n",
    "    return np.random.multinomial(1, theta).argmax()\n",
    "\n",
    "\n",
    "def word_indices(doc_sent_word_dict, sent_index):\n",
    "    \"\"\"\n",
    "    :param doc_sent_word_dict:\n",
    "    :param sent_index:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    sentence = doc_sent_word_dict[sent_index]\n",
    "    for idx in sentence:\n",
    "        yield idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from ast import literal_eval\n",
    "from collections import Counter\n",
    "from scipy.optimize import fmin_l_bfgs_b\n",
    "import optimizeTopicVectors as ot\n",
    "from preprocess import *\n",
    "\n",
    "def sampleFromDirichlet(alpha):\n",
    "    return np.random.dirichlet(alpha)\n",
    "\n",
    "\n",
    "def sampleFromCategorical(theta):\n",
    "    # theta = theta / np.sum(theta)\n",
    "    return np.random.multinomial(1, theta).argmax()\n",
    "\n",
    "\n",
    "def word_indices(doc_sent_word_dict, sent_index):\n",
    "    \"\"\"\n",
    "    :param doc_sent_word_dict:\n",
    "    :param sent_index:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    sentence = doc_sent_word_dict[sent_index]\n",
    "    for idx in sentence:\n",
    "        yield idx\n",
    "\n",
    "\n",
    "class ASUM_Gibbs_Sampler:\n",
    "    def __init__(self, wordVectors, sentimentVector, numTopics, alpha, beta, gamma, binary=0.5, max_sentence=50, numSentiments=2):\n",
    "        self.wordVectors = wordVectors # (V x H)\n",
    "        self.numTopics = numTopics\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.gamma = gamma\n",
    "        self.numSentiments = numSentiments\n",
    "        #self.MAX_VOCAB_SIZE = max_vocab_size\n",
    "        self.maxSentence = max_sentence\n",
    "        self.dimension = self.wordVectors.shape[1]  # H\n",
    "        self.binary = binary\n",
    "        self.sentimentVector = sentimentVector # (L x H)\n",
    "\n",
    "    def build_dataset(self, reviews, sentiment_list):\n",
    "        \"\"\"\n",
    "        :param reviews: 리뷰 데이터 [ [[문서1의 문장1],[문서1의 문장2]], [[문서2의 문장1],[문서2의 문장2]], ...]]\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        corpus = [word for review in reviews for sentence in review for word in sentence]\n",
    "        text = nltk.Text(corpus)\n",
    "        freq = nltk.FreqDist(text)\n",
    "        #keywords = [tup[0] for tup in freq.most_common(self.MAX_VOCAB_SIZE)]  # 많이 등장한 단어 선택\n",
    "        keywords = [tup[0] for tup in freq.most_common(self.wordVectors.shape[0])]  # 많이 등장한 단어 선택\n",
    "        word2idx = {}  # key : 단어, value : index\n",
    "        for index, key in enumerate(keywords):\n",
    "            word2idx[key] = index\n",
    "\n",
    "        idx2word = dict(zip(word2idx.values(), word2idx.keys()))  # key : index, value : 단어\n",
    "        doc_sent_word_dict = {}  # key: 문서 index, value : [[list of sent1 단어의 index], [list of sent2 단어의 index]...]\n",
    "        numSentence = {}  # key : 문서 index, value : 해당 문서의 문장수\n",
    "        wordCountSentence = {}  # key : 문서 index, value : 해당 문서의 각 문장별 word count\n",
    "        docSentiment = {}\n",
    "        for index, review in enumerate(reviews):\n",
    "            doc_sent_lst = []\n",
    "            doc_sent_count = []\n",
    "            for sent in review:\n",
    "                word_indices = [word2idx[word] for word in sent if word in word2idx]\n",
    "                doc_sent_lst.append(word_indices)\n",
    "                counts = Counter(word_indices)\n",
    "                doc_sent_count.append(counts)\n",
    "            numSentence[index] = len(doc_sent_lst)\n",
    "            doc_sent_word_dict[index] = doc_sent_lst\n",
    "            wordCountSentence[index] = doc_sent_count\n",
    "            docSentiment[index] = sentiment_list[index]\n",
    "\n",
    "        return word2idx, idx2word, doc_sent_word_dict, wordCountSentence, numSentence, docSentiment\n",
    "\n",
    "    def _initialize_(self, reviews, pos_neg_sentence_indices, pos_neg_sentiment_label, sentiment_list):\n",
    "        self.word2idx, self.idx2word, self.doc_sent_word_dict, self.wordCountSentence, \\\n",
    "        self.numSentence, self.docSentiment = self.build_dataset(reviews, sentiment_list)\n",
    "        self.numDocs = len(self.doc_sent_word_dict.keys())\n",
    "        self.vocabSize = len(self.word2idx.keys())\n",
    "        self.pos_neg_sentence_indices = pos_neg_sentence_indices\n",
    "        self.pos_neg_sentiment_label = pos_neg_sentiment_label\n",
    "        self.topicVectors = ot.orthogonal_matrix((self.numTopics, self.dimension))\n",
    "\n",
    "        # Pseudocounts\n",
    "        self.n_wkl = np.zeros((self.vocabSize, self.numTopics, self.numSentiments))  # 단어 i가 topic k, senti l로 할당된 수\n",
    "        self.n_kl = np.zeros((self.numTopics, self.numSentiments))  # topic k, senti l로 할당된 단어 수\n",
    "        self.ns_d = np.zeros((self.numDocs))  # 문서 d의 문장 수\n",
    "        self.ns_dkl = np.zeros((self.numDocs, self.numTopics, self.numSentiments))  # 문서 d에서 topic k, sentiment l로 할당된 문장 수\n",
    "        self.ns_dl = np.zeros((self.numDocs, self.numSentiments))  # 문서 d에서 topic k로 할당된 문장 수\n",
    "        self.topics = {}\n",
    "        self.sentiments = {}\n",
    "\n",
    "        alphaVec = self.alpha * np.ones(self.numTopics)\n",
    "        gammaVec = self.gamma * np.ones(self.numSentiments)\n",
    "\n",
    "        for d in range(self.numDocs):\n",
    "            topicDistribution = sampleFromDirichlet(alphaVec)\n",
    "            sentimentDistribution = np.zeros((self.numTopics, self.numSentiments))\n",
    "\n",
    "            for t in range(self.numTopics):\n",
    "                sentimentDistribution[t, :] = sampleFromDirichlet(gammaVec)\n",
    "\n",
    "            for m in range(self.numSentence[d]):\n",
    "                t = sampleFromCategorical(topicDistribution)\n",
    "                # s = sampleFromCategorical(sentimentDistribution[t, :])\n",
    "                # s = self.docSentiment[d]\n",
    "                pos_score = np.dot(self.sentimentVector,\n",
    "                                   self.wordVectors[self.doc_sent_word_dict[d][m]].T).sum(axis=1)\n",
    "                s = np.argmax(pos_score)\n",
    "                self.topics[(d, m)] = t  # d 문서의 m번째 문장의 topic\n",
    "                self.sentiments[(d, m)] = s  # d 문서의 m 번째 문장의 sentiment\n",
    "                self.ns_d[d] += 1\n",
    "                self.ns_dkl[d, t, s] += 1\n",
    "                self.ns_dl[d, s] += 1\n",
    "                for i, w in enumerate(word_indices(self.doc_sent_word_dict[d], m)):  # d번째 문서의 m번째 문장의 단어를 돌면서\n",
    "                    self.n_wkl[w, t, s] += 1  # w번째 단어가 topic은 t, sentiment s로 할당된 개수\n",
    "                    self.n_kl[t, s] += 1  # topic k, senti l로 할당된 단어 수\n",
    "\n",
    "    def updateTopicVectors(self, lamda = 0.01):\n",
    "        t = self.topicVectors # (K, H)\n",
    "        for i in range(self.numTopics):\n",
    "            x0 = t[i, :]\n",
    "            x, f, d = fmin_l_bfgs_b(ot.loss, x0, fprime=ot.grad, args=(self.n_wkl, wordVectors, lamda), maxiter=15000)\n",
    "            t[i, :] = x\n",
    "        self.topicVectors = t\n",
    "\n",
    "\n",
    "    def sampling(self, d, m):\n",
    "        t = self.topics[(d, m)]\n",
    "        s = self.sentiments[(d, m)]\n",
    "        self.ns_d[d] -= 1\n",
    "        self.ns_dkl[d, t, s] -= 1\n",
    "        self.ns_dl[d, s] -= 1\n",
    "        for i, w in enumerate(word_indices(self.doc_sent_word_dict[d], m)):\n",
    "            self.n_wkl[w, t, s] -= 1  # w번째 단어가 topic은 t, sentiment s로 할당된 개수\n",
    "            self.n_kl[t, s] -= 1  # topic k, senti l로 할당된 단어 수\n",
    "\n",
    "        firstFactor = np.ones((self.numTopics, self.numSentiments))\n",
    "\n",
    "        word_count = self.wordCountSentence[d][m]\n",
    "        for t in range(self.numTopics):\n",
    "            for s in range(self.numSentiments):\n",
    "                beta0 = self.n_kl[t][s] + self.beta\n",
    "                m0 = 0\n",
    "                for word in word_count.keys():\n",
    "                    betaw = self.n_wkl[word, t, s] + self.beta\n",
    "                    cnt = word_count[word]\n",
    "                    for i in range(cnt):\n",
    "                        firstFactor[t][s] *= (betaw + i) / (beta0 + m0)\n",
    "                        m0 += 1\n",
    "\n",
    "        # topic_similarity = ot.softmax(np.dot(self.topicVectors,\n",
    "        #                                      self.wordVectors[\n",
    "        #                                          self.doc_sent_word_dict[d][m]].T))  # ( K x num words in sentence)\n",
    "        # senti_similarity = ot.softmax(np.dot(self.sentimentVector,\n",
    "        #                                      self.wordVectors[\n",
    "        #                                          self.doc_sent_word_dict[d][m]].T))  # ( L x num words in sentence)\n",
    "        # vector_similarity = ot.softmax(np.dot(topic_similarity, senti_similarity.T))\n",
    "        #\n",
    "        # firstFactor = firstFactor *  vector_similarity # dim(K x L)\n",
    "\n",
    "        secondFactor = (self.ns_dl[d, :] + self.alpha) / \\\n",
    "                       (self.ns_d[d] + self.numTopics * self.alpha)  # dim(L x 1)\n",
    "\n",
    "        thirdFactor = (self.ns_dkl[d, :, :] + self.gamma) / \\\n",
    "                      (self.ns_dl[d] + self.numSentiments * self.gamma)[np.newaxis, :] #(K, L)\n",
    "\n",
    "        prob = np.ones((self.numTopics, self.numSentiments))\n",
    "        prob *= firstFactor * thirdFactor\n",
    "        prob *= secondFactor[np.newaxis,:]\n",
    "        prob /= np.sum(prob)\n",
    "\n",
    "        ind = sampleFromCategorical(prob.flatten())\n",
    "        t, s = np.unravel_index(ind, prob.shape)\n",
    "\n",
    "        self.topics[(d, m)] = t\n",
    "        self.sentiments[(d, m)] = s\n",
    "        self.ns_d[d] += 1\n",
    "        self.ns_dkl[d, t, s] += 1\n",
    "        self.ns_dl[d, s] += 1\n",
    "        for i, w in enumerate(word_indices(self.doc_sent_word_dict[d], m)):\n",
    "            self.n_wkl[w, t, s] += 1  # w번째 단어가 topic은 t, sentiment s로 할당된 개수\n",
    "            self.n_kl[t, s] += 1  # topic k, senti l로 할당된 단어 수\n",
    "\n",
    "    def calculatePhi(self):\n",
    "        firstFactor = (self.n_wkl + self.beta) / \\\n",
    "                      np.expand_dims(self.n_kl + self.n_wkl.shape[0] * self.beta, axis=0)\n",
    "\n",
    "        # topic_similarity = ot.softmax(np.dot(self.topicVectors,\n",
    "        #                                      self.wordVectors.T))  # ( K x V)\n",
    "        # senti_similarity = ot.softmax(np.dot(self.sentimentVector,\n",
    "        #                                      self.wordVectors.T))  # ( L x V)\n",
    "        # vector_similarity = ot.softmax(np.dot(topic_similarity, senti_similarity.T)) # K x L\n",
    "        #\n",
    "        # firstFactor = firstFactor * np.expand_dims(vector_similarity, axis=0)\n",
    "        # firstFactor /= firstFactor.sum()\n",
    "        return firstFactor\n",
    "\n",
    "    def calculateTheta(self):\n",
    "        secondFactor = (self.ns_dkl + self.alpha) / \\\n",
    "                       np.expand_dims(self.ns_dl + self.numTopics * self.alpha, axis=1)  # dim(K x 1)\n",
    "        secondFactor /= secondFactor.sum()\n",
    "        return secondFactor\n",
    "\n",
    "    def calculatePi(self):\n",
    "        thirdFactor = (self.ns_dl + self.gamma) / \\\n",
    "                      np.expand_dims(self.ns_d + self.numSentiments * self.gamma, axis=1)\n",
    "        thirdFactor /= thirdFactor.sum()\n",
    "        return thirdFactor\n",
    "\n",
    "\n",
    "    def getTopKWordsByLikelihood(self, K):\n",
    "        \"\"\"\n",
    "        Returns top K discriminative words for topic t and sentiment s\n",
    "        ie words v for which p(t, s | v) is maximum\n",
    "        \"\"\"\n",
    "        pseudocounts = np.copy(self.n_wkl)\n",
    "        normalizer = np.sum(pseudocounts, (1, 2))\n",
    "        pseudocounts /= normalizer[:, np.newaxis, np.newaxis]\n",
    "        for t in range(self.numTopics):\n",
    "            for s in range(self.numSentiments):\n",
    "                topWordIndices = pseudocounts[:, t, s].argsort()[-1:-(K + 1):-1]\n",
    "                # vocab = self.vectorizer.get_feature_names()\n",
    "                print(t, s, [self.idx2word[i] for i in topWordIndices])\n",
    "\n",
    "    def getTopKWordsByTS(self, K):\n",
    "        \"\"\"\n",
    "        K 개 sentiment별 top words\n",
    "        \"\"\"\n",
    "        topic_sentiment_arr = self.calculatePhi()\n",
    "        dic = {}\n",
    "        for t in range(self.numTopics):\n",
    "            for s in range(self.numSentiments):\n",
    "                index_list = np.argsort(-topic_sentiment_arr[:, t, s])[:10]\n",
    "                if s == 0:\n",
    "                    name = \"p\"\n",
    "                else:\n",
    "                    name = \"n\"\n",
    "                dic['topic_' + '{:02d}'.format(t + 1) + '_' + name] = [self.idx2word[index] for index in index_list]\n",
    "        return pd.DataFrame(dic)\n",
    "\n",
    "    def getTopKWordsByTopic(self, K):\n",
    "        dic = {}\n",
    "        phi = self.calculatePhi()\n",
    "        topic_arr = np.sum(phi, (2))\n",
    "        for t in range(self.numTopics):\n",
    "            index_list = np.argsort(-topic_arr[:, t])[:K]\n",
    "            dic[\"Topic\"+str(t+1)] = [self.idx2word[index] for index in index_list]\n",
    "        return pd.DataFrame(dic)\n",
    "\n",
    "    def getTopicSentimentDist(self, d):\n",
    "        theta = self.calculateTheta()[d]\n",
    "        return theta\n",
    "\n",
    "    def getDocSentimentDist(self, d):\n",
    "        pi = self.calculatePi()[d]\n",
    "        return pi\n",
    "\n",
    "    def getTopWordsBySenti(self, K):\n",
    "        dic = {}\n",
    "        phi = self.calculatePhi()\n",
    "        senti_arr = np.sum(phi, (1))\n",
    "        for s in range(self.numSentiments):\n",
    "            index_list = np.argsort(-senti_arr[:, s])[:K]\n",
    "            if s == 0:\n",
    "                name = \"p\"\n",
    "            else:\n",
    "                name = \"n\"\n",
    "            dic[\"Sentiment_\"+ name] = [self.idx2word[index] for index in index_list]\n",
    "        return pd.DataFrame(dic)\n",
    "\n",
    "    def classify_senti(self):\n",
    "        doc_sent_inference = []\n",
    "        for i in range(self.numDocs):\n",
    "            if i in self.pos_neg_sentence_indices:\n",
    "                doc_sent_inference.append(np.argmax(self.getDocSentimentDist(i)))\n",
    "        infer_arr = np.array(doc_sent_inference)\n",
    "        answer = np.array(self.pos_neg_sentiment_label)\n",
    "        return np.mean(infer_arr == answer)\n",
    "\n",
    "    def runASUM(self, reviews, maxIters=10):\n",
    "        for iteration in range(maxIters):\n",
    "            self.updateTopicVectors()\n",
    "            if (iteration + 1) % 2 == 0:\n",
    "                print(\"Starting iteration %d of %d\" % (iteration + 1, maxIters))\n",
    "                print(self.classify_senti())\n",
    "\n",
    "            for d in range(self.numDocs):\n",
    "                for m in range(self.numSentence[d]):\n",
    "                    self.sampling(d, m)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load data\n",
    "work_path = \"/media/hs-ubuntu/data/dataset/MasterThesis/STMD_data/\"\n",
    "data = pd.read_csv(work_path + \"preprocess_complete_Electronics.csv\")\n",
    "\n",
    "brand = ['Apple', 'Samsung','Canon']\n",
    "brand_df = data[data.brand.isin(brand)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#긍정, 부정 반반씩\n",
    "pos_reviews = brand_df[brand_df.overall >= 4]\n",
    "neg_reviews = brand_df[brand_df.overall <= 2]\n",
    "pos_sample = pos_reviews.sample(3500, random_state=23)\n",
    "neg_sample = neg_reviews.sample(3500, random_state=42)\n",
    "df = pd.concat([pos_sample, neg_sample], axis=0)\n",
    "df['preprocessed'] = df.preprocessed.apply(lambda row: literal_eval(row))\n",
    "df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hs-ubuntu/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "# brand_df = data[data['brand'] == brand]\n",
    "# brand_df.reset_index(drop=True, inplace=True)\n",
    "# brand_df['preprocessed'] = brand_df.preprocessed.apply(lambda row: literal_eval(row))\n",
    "# tagged_text_list = list(brand_df['reviewSentence_tagged'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# embedding\n",
    "from preprocess import *\n",
    "\n",
    "sentence_list, sentiment_label, sentence_senti_label, \\\n",
    "pos_neg_sentence_indices, pos_neg_sentiment_label, numSentence = prepare(df)\n",
    "\n",
    "documents, sentence_list_again, bigram\\\n",
    "= bigram_and_sentence(sentence_senti_label, sentence_list, numSentence, max_vocab=5000, threshold = 5, min_count = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 단어 분포\n",
    "corpus = [word for review in sentence_list_again for sent in review for word in sent]\n",
    "text = nltk.Text(corpus)\n",
    "freq = nltk.FreqDist(text)\n",
    "# keywords = [tup[0] for tup in freq.most_common(3000)] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5000"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(freq.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [02:41<00:00, 32.26s/it]\n"
     ]
    }
   ],
   "source": [
    "window = [2]\n",
    "size = [100]\n",
    "passes = 5\n",
    "for w in window:\n",
    "    for s in size:\n",
    "        model = Doc2Vec(dm=1, \n",
    "                        dm_mean=1, \n",
    "                        min_count=0, sample=1e-5,\n",
    "                        window=w, size=s, \n",
    "                        workers=multiprocessing.cpu_count(), \n",
    "                        alpha=0.025, min_alpha=0.025)\n",
    "        model.build_vocab(documents)\n",
    "\n",
    "        for epoch in tqdm(range(passes)):\n",
    "            random.shuffle(documents)\n",
    "            model.train(documents)\n",
    "            model.alpha -= 0.002  # decrease the learning rate\n",
    "            model.min_alpha = model.alpha  # fix the learning rate, no decay\n",
    "#             if (epoch + 1) % 5 ==0:\n",
    "#                 model.save(work_path + 'model_' + str(w) + '_' + str(s) + '_' + str(epoch+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5000"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model.index2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# model = Doc2Vec.load(work_path + 'model_3_100_10')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('amazon', 0.7968705296516418),\n",
       " ('agent', 0.7953909635543823),\n",
       " ('return', 0.7908092737197876),\n",
       " ('two_week', 0.7863162755966187),\n",
       " ('custom_servic', 0.7842612266540527),\n",
       " ('ago', 0.777485728263855),\n",
       " ('exchang', 0.7764937281608582),\n",
       " ('factori', 0.7723338007926941),\n",
       " ('within_day', 0.7692259550094604),\n",
       " ('month', 0.7646186947822571),\n",
       " ('telephon', 0.7642788887023926),\n",
       " ('send', 0.7582880258560181),\n",
       " ('told', 0.7575987577438354),\n",
       " ('dealer', 0.7546489238739014),\n",
       " ('receipt', 0.7521483898162842)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar([model.docvecs['negative']], topn=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('contest', 0.7410257458686829),\n",
       " ('shooter', 0.7270699739456177),\n",
       " ('creativ', 0.7251889705657959),\n",
       " ('defeat', 0.7168301343917847),\n",
       " ('bang_buck', 0.7161418199539185),\n",
       " ('con', 0.7043457627296448),\n",
       " ('easi_use', 0.6940544247627258),\n",
       " ('goal', 0.6927509307861328),\n",
       " ('size_weight', 0.6918964982032776),\n",
       " ('demand', 0.688834547996521),\n",
       " ('ultra', 0.6885380148887634),\n",
       " ('capabl', 0.6874716877937317),\n",
       " ('vivid', 0.6841638088226318),\n",
       " ('carri_around', 0.6841549873352051),\n",
       " ('hd_video', 0.683285117149353)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar([model.docvecs['positive']], topn=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wordVectors = np.zeros((len(model.index2word), model.vector_size))\n",
    "for index, word in enumerate(model.index2word):\n",
    "    wordVectors[index,:] = model[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sentimentVector = np.zeros((2, model.vector_size))\n",
    "sentimentVector[0,:] = model.docvecs['positive']\n",
    "sentimentVector[1,:] = model.docvecs['negative']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from ast import literal_eval\n",
    "from collections import Counter\n",
    "from scipy.optimize import fmin_l_bfgs_b\n",
    "import optimizeTopicVectors as ot\n",
    "from preprocess import *\n",
    "\n",
    "def sampleFromDirichlet(alpha):\n",
    "    return np.random.dirichlet(alpha)\n",
    "\n",
    "\n",
    "def sampleFromCategorical(theta):\n",
    "    # theta = theta / np.sum(theta)\n",
    "    return np.random.multinomial(1, theta).argmax()\n",
    "\n",
    "\n",
    "def word_indices(doc_sent_word_dict, sent_index):\n",
    "    \"\"\"\n",
    "    :param doc_sent_word_dict:\n",
    "    :param sent_index:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    sentence = doc_sent_word_dict[sent_index]\n",
    "    for idx in sentence:\n",
    "        yield idx\n",
    "\n",
    "\n",
    "class ASUM_Gibbs_Sampler:\n",
    "    def __init__(self, wordVectors, sentimentVector, numTopics, alpha, beta, gamma, binary=0.5, max_sentence=50, numSentiments=2):\n",
    "        self.wordVectors = wordVectors # (V x H)\n",
    "        self.numTopics = numTopics\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.gamma = gamma\n",
    "        self.numSentiments = numSentiments\n",
    "        #self.MAX_VOCAB_SIZE = max_vocab_size\n",
    "        self.maxSentence = max_sentence\n",
    "        self.dimension = self.wordVectors.shape[1]  # H\n",
    "        self.binary = binary\n",
    "        self.sentimentVector = sentimentVector # (L x H)\n",
    "\n",
    "    def build_dataset(self, reviews, sentiment_list):\n",
    "        \"\"\"\n",
    "        :param reviews: 리뷰 데이터 [ [[문서1의 문장1],[문서1의 문장2]], [[문서2의 문장1],[문서2의 문장2]], ...]]\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        corpus = [word for review in reviews for sentence in review for word in sentence]\n",
    "        text = nltk.Text(corpus)\n",
    "        freq = nltk.FreqDist(text)\n",
    "        #keywords = [tup[0] for tup in freq.most_common(self.MAX_VOCAB_SIZE)]  # 많이 등장한 단어 선택\n",
    "        keywords = [tup[0] for tup in freq.most_common(self.wordVectors.shape[0])]  # 많이 등장한 단어 선택\n",
    "        word2idx = {}  # key : 단어, value : index\n",
    "        for index, key in enumerate(keywords):\n",
    "            word2idx[key] = index\n",
    "\n",
    "        idx2word = dict(zip(word2idx.values(), word2idx.keys()))  # key : index, value : 단어\n",
    "        doc_sent_word_dict = {}  # key: 문서 index, value : [[list of sent1 단어의 index], [list of sent2 단어의 index]...]\n",
    "        numSentence = {}  # key : 문서 index, value : 해당 문서의 문장수\n",
    "        wordCountSentence = {}  # key : 문서 index, value : 해당 문서의 각 문장별 word count\n",
    "        docSentiment = {}\n",
    "        for index, review in enumerate(reviews):\n",
    "            doc_sent_lst = []\n",
    "            doc_sent_count = []\n",
    "            for sent in review:\n",
    "                word_indices = [word2idx[word] for word in sent if word in word2idx]\n",
    "                doc_sent_lst.append(word_indices)\n",
    "                counts = Counter(word_indices)\n",
    "                doc_sent_count.append(counts)\n",
    "            numSentence[index] = len(doc_sent_lst)\n",
    "            doc_sent_word_dict[index] = doc_sent_lst\n",
    "            wordCountSentence[index] = doc_sent_count\n",
    "            docSentiment[index] = sentiment_list[index]\n",
    "\n",
    "        return word2idx, idx2word, doc_sent_word_dict, wordCountSentence, numSentence, docSentiment\n",
    "\n",
    "    def _initialize_(self, reviews, pos_neg_sentence_indices, pos_neg_sentiment_label, sentiment_list):\n",
    "        self.word2idx, self.idx2word, self.doc_sent_word_dict, self.wordCountSentence, \\\n",
    "        self.numSentence, self.docSentiment = self.build_dataset(reviews, sentiment_list)\n",
    "        self.numDocs = len(self.doc_sent_word_dict.keys())\n",
    "        self.vocabSize = len(self.word2idx.keys())\n",
    "        self.pos_neg_sentence_indices = pos_neg_sentence_indices\n",
    "        self.pos_neg_sentiment_label = pos_neg_sentiment_label\n",
    "        self.topicVectors = ot.orthogonal_matrix((self.numTopics, self.dimension))\n",
    "\n",
    "        # Pseudocounts\n",
    "        self.n_wkl = np.zeros((self.vocabSize, self.numTopics, self.numSentiments))  # 단어 i가 topic k, senti l로 할당된 수\n",
    "        self.n_kl = np.zeros((self.numTopics, self.numSentiments))  # topic k, senti l로 할당된 단어 수\n",
    "        self.ns_d = np.zeros((self.numDocs))  # 문서 d의 문장 수\n",
    "        self.ns_dkl = np.zeros((self.numDocs, self.numTopics, self.numSentiments))  # 문서 d에서 topic k, sentiment l로 할당된 문장 수\n",
    "        self.ns_dl = np.zeros((self.numDocs, self.numSentiments))  # 문서 d에서 topic k로 할당된 문장 수\n",
    "        self.topics = {}\n",
    "        self.sentiments = {}\n",
    "\n",
    "        alphaVec = self.alpha * np.ones(self.numTopics)\n",
    "        gammaVec = self.gamma * np.ones(self.numSentiments)\n",
    "\n",
    "        for d in range(self.numDocs):\n",
    "            topicDistribution = sampleFromDirichlet(alphaVec)\n",
    "            sentimentDistribution = np.zeros((self.numTopics, self.numSentiments))\n",
    "\n",
    "            for t in range(self.numTopics):\n",
    "                sentimentDistribution[t, :] = sampleFromDirichlet(gammaVec)\n",
    "\n",
    "            for m in range(self.numSentence[d]):\n",
    "                t = sampleFromCategorical(topicDistribution)\n",
    "                # s = sampleFromCategorical(sentimentDistribution[t, :])\n",
    "                # s = self.docSentiment[d]\n",
    "                pos_score = np.dot(self.sentimentVector,\n",
    "                                   self.wordVectors[self.doc_sent_word_dict[d][m]].T).sum(axis=1)\n",
    "                s = np.argmax(pos_score)\n",
    "                self.topics[(d, m)] = t  # d 문서의 m번째 문장의 topic\n",
    "                self.sentiments[(d, m)] = s  # d 문서의 m 번째 문장의 sentiment\n",
    "                self.ns_d[d] += 1\n",
    "                self.ns_dkl[d, t, s] += 1\n",
    "                self.ns_dl[d, s] += 1\n",
    "                for i, w in enumerate(word_indices(self.doc_sent_word_dict[d], m)):  # d번째 문서의 m번째 문장의 단어를 돌면서\n",
    "                    self.n_wkl[w, t, s] += 1  # w번째 단어가 topic은 t, sentiment s로 할당된 개수\n",
    "                    self.n_kl[t, s] += 1  # topic k, senti l로 할당된 단어 수\n",
    "\n",
    "    def updateTopicVectors(self, lamda = 0.01):\n",
    "        t = self.topicVectors # (K, H)\n",
    "        for i in range(self.numTopics):\n",
    "            x0 = t[i, :]\n",
    "            x, f, d = fmin_l_bfgs_b(ot.loss, x0, fprime=ot.grad, args=(self.n_wkl, wordVectors, lamda), maxiter=15000)\n",
    "            t[i, :] = x\n",
    "        self.topicVectors = t\n",
    "\n",
    "\n",
    "    def sampling(self, d, m):\n",
    "        t = self.topics[(d, m)]\n",
    "        s = self.sentiments[(d, m)]\n",
    "        self.ns_d[d] -= 1\n",
    "        self.ns_dkl[d, t, s] -= 1\n",
    "        self.ns_dl[d, s] -= 1\n",
    "        for i, w in enumerate(word_indices(self.doc_sent_word_dict[d], m)):\n",
    "            self.n_wkl[w, t, s] -= 1  # w번째 단어가 topic은 t, sentiment s로 할당된 개수\n",
    "            self.n_kl[t, s] -= 1  # topic k, senti l로 할당된 단어 수\n",
    "\n",
    "        firstFactor = np.ones((self.numTopics, self.numSentiments))\n",
    "\n",
    "        word_count = self.wordCountSentence[d][m]\n",
    "        for t in range(self.numTopics):\n",
    "            for s in range(self.numSentiments):\n",
    "                beta0 = self.n_kl[t][s] + self.beta\n",
    "                m0 = 0\n",
    "                for word in word_count.keys():\n",
    "                    betaw = self.n_wkl[word, t, s] + self.beta\n",
    "                    cnt = word_count[word]\n",
    "                    for i in range(cnt):\n",
    "                        firstFactor[t][s] *= (betaw + i) / (beta0 + m0)\n",
    "                        m0 += 1\n",
    "\n",
    "        # topic_similarity = ot.softmax(np.dot(self.topicVectors,\n",
    "        #                                      self.wordVectors[\n",
    "        #                                          self.doc_sent_word_dict[d][m]].T))  # ( K x num words in sentence)\n",
    "        # senti_similarity = ot.softmax(np.dot(self.sentimentVector,\n",
    "        #                                      self.wordVectors[\n",
    "        #                                          self.doc_sent_word_dict[d][m]].T))  # ( L x num words in sentence)\n",
    "        # vector_similarity = ot.softmax(np.dot(topic_similarity, senti_similarity.T))\n",
    "        #\n",
    "        # firstFactor = firstFactor *  vector_similarity # dim(K x L)\n",
    "\n",
    "        secondFactor = (self.ns_dl[d, :] + self.alpha) / \\\n",
    "                       (self.ns_d[d] + self.numTopics * self.alpha)  # dim(L x 1)\n",
    "\n",
    "        thirdFactor = (self.ns_dkl[d, :, :] + self.gamma) / \\\n",
    "                      (self.ns_dl[d] + self.numSentiments * self.gamma)[np.newaxis, :] #(K, L)\n",
    "\n",
    "        prob = np.ones((self.numTopics, self.numSentiments))\n",
    "        prob *= firstFactor * thirdFactor\n",
    "        prob *= secondFactor[np.newaxis,:]\n",
    "        prob /= np.sum(prob)\n",
    "\n",
    "        ind = sampleFromCategorical(prob.flatten())\n",
    "        t, s = np.unravel_index(ind, prob.shape)\n",
    "\n",
    "        self.topics[(d, m)] = t\n",
    "        self.sentiments[(d, m)] = s\n",
    "        self.ns_d[d] += 1\n",
    "        self.ns_dkl[d, t, s] += 1\n",
    "        self.ns_dl[d, s] += 1\n",
    "        for i, w in enumerate(word_indices(self.doc_sent_word_dict[d], m)):\n",
    "            self.n_wkl[w, t, s] += 1  # w번째 단어가 topic은 t, sentiment s로 할당된 개수\n",
    "            self.n_kl[t, s] += 1  # topic k, senti l로 할당된 단어 수\n",
    "\n",
    "    def calculatePhi(self):\n",
    "        firstFactor = (self.n_wkl + self.beta) / \\\n",
    "                      np.expand_dims(self.n_kl + self.n_wkl.shape[0] * self.beta, axis=0)\n",
    "\n",
    "        # topic_similarity = ot.softmax(np.dot(self.topicVectors,\n",
    "        #                                      self.wordVectors.T))  # ( K x V)\n",
    "        # senti_similarity = ot.softmax(np.dot(self.sentimentVector,\n",
    "        #                                      self.wordVectors.T))  # ( L x V)\n",
    "        # vector_similarity = ot.softmax(np.dot(topic_similarity, senti_similarity.T)) # K x L\n",
    "        #\n",
    "        # firstFactor = firstFactor * np.expand_dims(vector_similarity, axis=0)\n",
    "        # firstFactor /= firstFactor.sum()\n",
    "        return firstFactor\n",
    "\n",
    "    def calculateTheta(self):\n",
    "        secondFactor = (self.ns_dkl + self.alpha) / \\\n",
    "                       np.expand_dims(self.ns_dl + self.numTopics * self.alpha, axis=1)  # dim(K x 1)\n",
    "        secondFactor /= secondFactor.sum()\n",
    "        return secondFactor\n",
    "\n",
    "    def calculatePi(self):\n",
    "        thirdFactor = (self.ns_dl + self.gamma) / \\\n",
    "                      np.expand_dims(self.ns_d + self.numSentiments * self.gamma, axis=1)\n",
    "        thirdFactor /= thirdFactor.sum()\n",
    "        return thirdFactor\n",
    "\n",
    "\n",
    "    def getTopKWordsByLikelihood(self, K):\n",
    "        \"\"\"\n",
    "        Returns top K discriminative words for topic t and sentiment s\n",
    "        ie words v for which p(t, s | v) is maximum\n",
    "        \"\"\"\n",
    "        pseudocounts = np.copy(self.n_wkl)\n",
    "        normalizer = np.sum(pseudocounts, (1, 2))\n",
    "        pseudocounts /= normalizer[:, np.newaxis, np.newaxis]\n",
    "        for t in range(self.numTopics):\n",
    "            for s in range(self.numSentiments):\n",
    "                topWordIndices = pseudocounts[:, t, s].argsort()[-1:-(K + 1):-1]\n",
    "                # vocab = self.vectorizer.get_feature_names()\n",
    "                print(t, s, [self.idx2word[i] for i in topWordIndices])\n",
    "\n",
    "    def getTopKWordsByTS(self, K):\n",
    "        \"\"\"\n",
    "        K 개 sentiment별 top words\n",
    "        \"\"\"\n",
    "        topic_sentiment_arr = self.calculatePhi()\n",
    "        dic = {}\n",
    "        for t in range(self.numTopics):\n",
    "            for s in range(self.numSentiments):\n",
    "                index_list = np.argsort(-topic_sentiment_arr[:, t, s])[:10]\n",
    "                if s == 0:\n",
    "                    name = \"p\"\n",
    "                else:\n",
    "                    name = \"n\"\n",
    "                dic['topic_' + '{:02d}'.format(t + 1) + '_' + name] = [self.idx2word[index] for index in index_list]\n",
    "        return pd.DataFrame(dic)\n",
    "\n",
    "    def getTopKWordsByTopic(self, K):\n",
    "        dic = {}\n",
    "        phi = self.calculatePhi()\n",
    "        topic_arr = np.sum(phi, (2))\n",
    "        for t in range(self.numTopics):\n",
    "            index_list = np.argsort(-topic_arr[:, t])[:K]\n",
    "            dic[\"Topic\"+str(t+1)] = [self.idx2word[index] for index in index_list]\n",
    "        return pd.DataFrame(dic)\n",
    "\n",
    "    def getTopicSentimentDist(self, d):\n",
    "        theta = self.calculateTheta()[d]\n",
    "        return theta\n",
    "\n",
    "    def getDocSentimentDist(self, d):\n",
    "        pi = self.calculatePi()[d]\n",
    "        return pi\n",
    "\n",
    "    def getTopWordsBySenti(self, K):\n",
    "        dic = {}\n",
    "        phi = self.calculatePhi()\n",
    "        senti_arr = np.sum(phi, (1))\n",
    "        for s in range(self.numSentiments):\n",
    "            index_list = np.argsort(-senti_arr[:, s])[:K]\n",
    "            if s == 0:\n",
    "                name = \"p\"\n",
    "            else:\n",
    "                name = \"n\"\n",
    "            dic[\"Sentiment_\"+ name] = [self.idx2word[index] for index in index_list]\n",
    "        return pd.DataFrame(dic)\n",
    "\n",
    "    def classify_senti(self):\n",
    "        doc_sent_inference = []\n",
    "        for i in range(self.numDocs):\n",
    "            if i in self.pos_neg_sentence_indices:\n",
    "                doc_sent_inference.append(np.argmax(self.getDocSentimentDist(i)))\n",
    "        infer_arr = np.array(doc_sent_inference)\n",
    "        answer = np.array(self.pos_neg_sentiment_label)\n",
    "        return np.mean(infer_arr == answer)\n",
    "\n",
    "    def runASUM(self, reviews, maxIters=10):\n",
    "        for iteration in range(maxIters):\n",
    "            self.updateTopicVectors()\n",
    "            if (iteration + 1) % 2 == 0:\n",
    "                print(\"Starting iteration %d of %d\" % (iteration + 1, maxIters))\n",
    "                print(self.classify_senti())\n",
    "\n",
    "            for d in range(self.numDocs):\n",
    "                for m in range(self.numSentence[d]):\n",
    "                    self.sampling(d, m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class STMD_Gibbs_Sampler:\n",
    "    def __init__(self, wordVectors, sentimentVector, numTopics, alpha, beta, gamma, binary=0.5, max_sentence=50, numSentiments=2):\n",
    "        self.wordVectors = wordVectors # (V x H)\n",
    "        self.numTopics = numTopics\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.gamma = gamma\n",
    "        self.numSentiments = numSentiments\n",
    "        #self.MAX_VOCAB_SIZE = max_vocab_size\n",
    "        self.maxSentence = max_sentence\n",
    "        self.dimension = self.wordVectors.shape[1]  # H\n",
    "        self.binary = binary\n",
    "        self.sentimentVector = sentimentVector # (L x H)\n",
    "\n",
    "    def build_dataset(self, reviews, sentiment_list):\n",
    "        \"\"\"\n",
    "        :param reviews: 리뷰 데이터 [ [[문서1의 문장1],[문서1의 문장2]], [[문서2의 문장1],[문서2의 문장2]], ...]]\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        corpus = [word for review in reviews for sentence in review for word in sentence]\n",
    "        text = nltk.Text(corpus)\n",
    "        freq = nltk.FreqDist(text)\n",
    "        #keywords = [tup[0] for tup in freq.most_common(self.MAX_VOCAB_SIZE)]  # 많이 등장한 단어 선택\n",
    "        keywords = [tup[0] for tup in freq.most_common(self.wordVectors.shape[0])]  # 많이 등장한 단어 선택\n",
    "        word2idx = {}  # key : 단어, value : index\n",
    "        for index, key in enumerate(keywords):\n",
    "            word2idx[key] = index\n",
    "\n",
    "        idx2word = dict(zip(word2idx.values(), word2idx.keys()))  # key : index, value : 단어\n",
    "        doc_sent_word_dict = {}  # key: 문서 index, value : [[list of sent1 단어의 index], [list of sent2 단어의 index]...]\n",
    "        numSentence = {}  # key : 문서 index, value : 해당 문서의 문장수\n",
    "        wordCountSentence = {}  # key : 문서 index, value : 해당 문서의 각 문장별 word count\n",
    "        docSentiment = {}\n",
    "        for index, review in enumerate(reviews):\n",
    "            doc_sent_lst = []\n",
    "            doc_sent_count = []\n",
    "            for sent in review:\n",
    "                word_indices = [word2idx[word] for word in sent if word in word2idx]\n",
    "                doc_sent_lst.append(word_indices)\n",
    "                counts = Counter(word_indices)\n",
    "                doc_sent_count.append(counts)\n",
    "            numSentence[index] = len(doc_sent_lst)\n",
    "            doc_sent_word_dict[index] = doc_sent_lst\n",
    "            wordCountSentence[index] = doc_sent_count\n",
    "            docSentiment[index] = sentiment_list[index]\n",
    "\n",
    "        return word2idx, idx2word, doc_sent_word_dict, wordCountSentence, numSentence, docSentiment\n",
    "\n",
    "    def _initialize_(self, reviews, pos_neg_sentence_indices, pos_neg_sentiment_label, sentiment_list):\n",
    "        self.word2idx, self.idx2word, self.doc_sent_word_dict, self.wordCountSentence, \\\n",
    "        self.numSentence, self.docSentiment = self.build_dataset(reviews, sentiment_list)\n",
    "        self.numDocs = len(self.doc_sent_word_dict.keys())\n",
    "        self.vocabSize = len(self.word2idx.keys())\n",
    "        self.pos_neg_sentence_indices = pos_neg_sentence_indices\n",
    "        self.pos_neg_sentiment_label = pos_neg_sentiment_label\n",
    "        self.topicVectors = ot.orthogonal_matrix((self.numTopics, self.dimension))\n",
    "\n",
    "        # Pseudocounts\n",
    "        self.n_wkl = np.zeros((self.vocabSize, self.numTopics, self.numSentiments))  # 단어 i가 topic k, senti l로 할당된 수\n",
    "        self.n_kl = np.zeros((self.numTopics, self.numSentiments))  # topic k, senti l로 할당된 단어 수\n",
    "        self.ns_d = np.zeros((self.numDocs))  # 문서 d의 문장 수\n",
    "        self.ns_dkl = np.zeros((self.numDocs, self.numTopics, self.numSentiments))  # 문서 d에서 topic k, sentiment l로 할당된 문장 수\n",
    "        self.ns_dl = np.zeros((self.numDocs, self.numSentiments))  # 문서 d에서 topic k로 할당된 문장 수\n",
    "        self.topics = {}\n",
    "        self.sentiments = {}\n",
    "\n",
    "        alphaVec = self.alpha * np.ones(self.numTopics)\n",
    "        gammaVec = self.gamma * np.ones(self.numSentiments)\n",
    "\n",
    "        for d in range(self.numDocs):\n",
    "            topicDistribution = sampleFromDirichlet(alphaVec)\n",
    "            sentimentDistribution = np.zeros((self.numTopics, self.numSentiments))\n",
    "\n",
    "            for t in range(self.numTopics):\n",
    "                sentimentDistribution[t, :] = sampleFromDirichlet(gammaVec)\n",
    "\n",
    "            for m in range(self.numSentence[d]):\n",
    "                t = sampleFromCategorical(topicDistribution)\n",
    "                # s = sampleFromCategorical(sentimentDistribution[t, :])\n",
    "                # s = self.docSentiment[d]\n",
    "                pos_score = np.dot(self.sentimentVector,\n",
    "                                   self.wordVectors[self.doc_sent_word_dict[d][m]].T).sum(axis=1)\n",
    "                s = np.argmax(pos_score)\n",
    "                self.topics[(d, m)] = t  # d 문서의 m번째 문장의 topic\n",
    "                self.sentiments[(d, m)] = s  # d 문서의 m 번째 문장의 sentiment\n",
    "                self.ns_d[d] += 1\n",
    "                self.ns_dkl[d, t, s] += 1\n",
    "                self.ns_dl[d, s] += 1\n",
    "                for i, w in enumerate(word_indices(self.doc_sent_word_dict[d], m)):  # d번째 문서의 m번째 문장의 단어를 돌면서\n",
    "                    self.n_wkl[w, t, s] += 1  # w번째 단어가 topic은 t, sentiment s로 할당된 개수\n",
    "                    self.n_kl[t, s] += 1  # topic k, senti l로 할당된 단어 수\n",
    "\n",
    "    def updateTopicVectors(self, lamda = 0.01):\n",
    "        t = self.topicVectors # (K, H)\n",
    "        for i in range(self.numTopics):\n",
    "            x0 = t[i, :]\n",
    "            x, f, d = fmin_l_bfgs_b(ot.loss, x0, fprime=ot.grad, args=(self.n_wkl, wordVectors, lamda), maxiter=15000)\n",
    "            t[i, :] = x\n",
    "        self.topicVectors = t\n",
    "\n",
    "\n",
    "    def sampling(self, d, m):\n",
    "        t = self.topics[(d, m)]\n",
    "        s = self.sentiments[(d, m)]\n",
    "        self.ns_d[d] -= 1\n",
    "        self.ns_dkl[d, t, s] -= 1\n",
    "        self.ns_dl[d, s] -= 1\n",
    "        for i, w in enumerate(word_indices(self.doc_sent_word_dict[d], m)):\n",
    "            self.n_wkl[w, t, s] -= 1  # w번째 단어가 topic은 t, sentiment s로 할당된 개수\n",
    "            self.n_kl[t, s] -= 1  # topic k, senti l로 할당된 단어 수\n",
    "\n",
    "        firstFactor = np.ones((self.numTopics, self.numSentiments))\n",
    "\n",
    "        word_count = self.wordCountSentence[d][m]\n",
    "        for t in range(self.numTopics):\n",
    "            for s in range(self.numSentiments):\n",
    "                beta0 = self.n_kl[t][s] + self.beta\n",
    "                m0 = 0\n",
    "                for word in word_count.keys():\n",
    "                    betaw = self.n_wkl[word, t, s] + self.beta\n",
    "                    cnt = word_count[word]\n",
    "                    for i in range(cnt):\n",
    "                        firstFactor[t][s] *= (betaw + i) / (beta0 + m0)\n",
    "                        m0 += 1\n",
    "\n",
    "        topic_similarity = ot.softmax(np.dot(self.topicVectors,\n",
    "                                             self.wordVectors[\n",
    "                                                 self.doc_sent_word_dict[d][m]].T))  # ( K x num words in sentence)\n",
    "        senti_similarity = ot.softmax(np.dot(self.sentimentVector,\n",
    "                                             self.wordVectors[\n",
    "                                                 self.doc_sent_word_dict[d][m]].T))  # ( L x num words in sentence)\n",
    "        vector_similarity = ot.softmax(np.dot(topic_similarity, senti_similarity.T))\n",
    "\n",
    "        firstFactor = firstFactor *  vector_similarity # dim(K x L)\n",
    "\n",
    "        secondFactor = (self.ns_dl[d, :] + self.alpha) / \\\n",
    "                       (self.ns_d[d] + self.numTopics * self.alpha)  # dim(L x 1)\n",
    "\n",
    "        thirdFactor = (self.ns_dkl[d, :, :] + self.gamma) / \\\n",
    "                      (self.ns_dl[d] + self.numSentiments * self.gamma)[np.newaxis, :] #(K, L)\n",
    "\n",
    "        prob = np.ones((self.numTopics, self.numSentiments))\n",
    "        prob *= firstFactor * thirdFactor\n",
    "        prob *= secondFactor[np.newaxis,:]\n",
    "        prob /= np.sum(prob)\n",
    "\n",
    "        ind = sampleFromCategorical(prob.flatten())\n",
    "        t, s = np.unravel_index(ind, prob.shape)\n",
    "\n",
    "        self.topics[(d, m)] = t\n",
    "        self.sentiments[(d, m)] = s\n",
    "        self.ns_d[d] += 1\n",
    "        self.ns_dkl[d, t, s] += 1\n",
    "        self.ns_dl[d, s] += 1\n",
    "        for i, w in enumerate(word_indices(self.doc_sent_word_dict[d], m)):\n",
    "            self.n_wkl[w, t, s] += 1  # w번째 단어가 topic은 t, sentiment s로 할당된 개수\n",
    "            self.n_kl[t, s] += 1  # topic k, senti l로 할당된 단어 수\n",
    "\n",
    "    def calculatePhi(self):\n",
    "        firstFactor = (self.n_wkl + self.beta) / \\\n",
    "                      np.expand_dims(self.n_kl + self.n_wkl.shape[0] * self.beta, axis=0)\n",
    "\n",
    "        topic_similarity = ot.softmax(np.dot(self.topicVectors,\n",
    "                                             self.wordVectors.T))  # ( K x V)\n",
    "        senti_similarity = ot.softmax(np.dot(self.sentimentVector,\n",
    "                                             self.wordVectors.T))  # ( L x V)\n",
    "        vector_similarity = ot.softmax(np.dot(topic_similarity, senti_similarity.T)) # K x L\n",
    "\n",
    "        firstFactor = firstFactor * np.expand_dims(vector_similarity, axis=0)\n",
    "        firstFactor /= firstFactor.sum()\n",
    "        return firstFactor\n",
    "\n",
    "    def calculateTheta(self):\n",
    "        secondFactor = (self.ns_dkl + self.alpha) / \\\n",
    "                       np.expand_dims(self.ns_dl + self.numTopics * self.alpha, axis=1)  # dim(K x 1)\n",
    "        secondFactor /= secondFactor.sum()\n",
    "        return secondFactor\n",
    "\n",
    "    def calculatePi(self):\n",
    "        thirdFactor = (self.ns_dl + self.gamma) / \\\n",
    "                      np.expand_dims(self.ns_d + self.numSentiments * self.gamma, axis=1)\n",
    "        thirdFactor /= thirdFactor.sum()\n",
    "        return thirdFactor\n",
    "\n",
    "\n",
    "    def getTopKWordsByLikelihood(self, K):\n",
    "        \"\"\"\n",
    "        Returns top K discriminative words for topic t and sentiment s\n",
    "        ie words v for which p(t, s | v) is maximum\n",
    "        \"\"\"\n",
    "        pseudocounts = np.copy(self.n_wkl)\n",
    "        normalizer = np.sum(pseudocounts, (1, 2))\n",
    "        pseudocounts /= normalizer[:, np.newaxis, np.newaxis]\n",
    "        for t in range(self.numTopics):\n",
    "            for s in range(self.numSentiments):\n",
    "                topWordIndices = pseudocounts[:, t, s].argsort()[-1:-(K + 1):-1]\n",
    "                # vocab = self.vectorizer.get_feature_names()\n",
    "                print(t, s, [self.idx2word[i] for i in topWordIndices])\n",
    "\n",
    "    def getTopKWordsByTS(self, K):\n",
    "        \"\"\"\n",
    "        K 개 sentiment별 top words\n",
    "        \"\"\"\n",
    "        topic_sentiment_arr = self.calculatePhi()\n",
    "        dic = {}\n",
    "        for t in range(self.numTopics):\n",
    "            for s in range(self.numSentiments):\n",
    "                index_list = np.argsort(-topic_sentiment_arr[:, t, s])[:10]\n",
    "                if s == 0:\n",
    "                    name = \"p\"\n",
    "                else:\n",
    "                    name = \"n\"\n",
    "                dic['topic_' + '{:02d}'.format(t + 1) + '_' + name] = [self.idx2word[index] for index in index_list]\n",
    "        return pd.DataFrame(dic)\n",
    "\n",
    "    def getTopKWordsByTopic(self, K):\n",
    "        dic = {}\n",
    "        phi = self.calculatePhi()\n",
    "        topic_arr = np.sum(phi, (2))\n",
    "        for t in range(self.numTopics):\n",
    "            index_list = np.argsort(-topic_arr[:, t])[:K]\n",
    "            dic[\"Topic\"+str(t+1)] = [self.idx2word[index] for index in index_list]\n",
    "        return pd.DataFrame(dic)\n",
    "\n",
    "    def getTopicSentimentDist(self, d):\n",
    "        theta = self.calculateTheta()[d]\n",
    "        return theta\n",
    "\n",
    "    def getDocSentimentDist(self, d):\n",
    "        pi = self.calculatePi()[d]\n",
    "        return pi\n",
    "\n",
    "    def getTopWordsBySenti(self, K):\n",
    "        dic = {}\n",
    "        phi = self.calculatePhi()\n",
    "        senti_arr = np.sum(phi, (1))\n",
    "        for s in range(self.numSentiments):\n",
    "            index_list = np.argsort(-senti_arr[:, s])[:K]\n",
    "            if s == 0:\n",
    "                name = \"p\"\n",
    "            else:\n",
    "                name = \"n\"\n",
    "            dic[\"Sentiment_\"+ name] = [self.idx2word[index] for index in index_list]\n",
    "        return pd.DataFrame(dic)\n",
    "\n",
    "    def classify_senti(self):\n",
    "        doc_sent_inference = []\n",
    "        for i in range(self.numDocs):\n",
    "            if i in self.pos_neg_sentence_indices:\n",
    "                doc_sent_inference.append(np.argmax(self.getDocSentimentDist(i)))\n",
    "        infer_arr = np.array(doc_sent_inference)\n",
    "        answer = np.array(self.pos_neg_sentiment_label)\n",
    "        return np.mean(infer_arr == answer)\n",
    "\n",
    "    def runSTMD(self, reviews, maxIters=10):\n",
    "        for iteration in range(maxIters):\n",
    "            self.updateTopicVectors()\n",
    "            if (iteration + 1) % 2 == 0:\n",
    "                print(\"Starting iteration %d of %d\" % (iteration + 1, maxIters))\n",
    "                print(self.classify_senti())\n",
    "\n",
    "            for d in range(self.numDocs):\n",
    "                for m in range(self.numSentence[d]):\n",
    "                    self.sampling(d, m)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# sampler = STMD_Gibbs_Sampler(wordVectors, sentimentVector, numTopics=50, alpha=0.01, beta=0.001, gamma=1, numSentiments=2)\n",
    "sampler = ASUM_Gibbs_Sampler(wordVectors, sentimentVector, numTopics=10, alpha=0.01, beta=0.001, gamma=1, numSentiments=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sampler._initialize_(sentence_list_again, pos_neg_sentence_indices, pos_neg_sentiment_label, sentiment_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting iteration 2 of 10\n",
      "0.749571428571\n",
      "Starting iteration 4 of 10\n",
      "0.749857142857\n",
      "Starting iteration 6 of 10\n",
      "0.742\n",
      "Starting iteration 8 of 10\n",
      "0.743571428571\n",
      "Starting iteration 10 of 10\n",
      "0.739857142857\n",
      "CPU times: user 4min 26s, sys: 268 ms, total: 4min 26s\n",
      "Wall time: 3min 54s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "sampler.runASUM(sentence_list_again, maxIters= 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "samp = STMD_Gibbs_Sampler(wordVectors, sentimentVector, numTopics=10, alpha=0.01, beta=0.001, gamma=1, numSentiments=2)\n",
    "samp._initialize_(sentence_list_again, pos_neg_sentence_indices, pos_neg_sentiment_label, sentiment_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting iteration 2 of 10\n",
      "0.756571428571\n",
      "Starting iteration 4 of 10\n",
      "0.755285714286\n",
      "Starting iteration 6 of 10\n",
      "0.756571428571\n",
      "Starting iteration 8 of 10\n",
      "0.750714285714\n",
      "Starting iteration 10 of 10\n",
      "0.750428571429\n"
     ]
    }
   ],
   "source": [
    "samp.runSTMD(sentence_list_again, maxIters= 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic1</th>\n",
       "      <th>Topic10</th>\n",
       "      <th>Topic2</th>\n",
       "      <th>Topic3</th>\n",
       "      <th>Topic4</th>\n",
       "      <th>Topic5</th>\n",
       "      <th>Topic6</th>\n",
       "      <th>Topic7</th>\n",
       "      <th>Topic8</th>\n",
       "      <th>Topic9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>camera</td>\n",
       "      <td>use</td>\n",
       "      <td>use</td>\n",
       "      <td>use</td>\n",
       "      <td>camera</td>\n",
       "      <td>tv</td>\n",
       "      <td>tv</td>\n",
       "      <td>camera</td>\n",
       "      <td>use</td>\n",
       "      <td>camera</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>love</td>\n",
       "      <td>one</td>\n",
       "      <td>tv</td>\n",
       "      <td>tablet</td>\n",
       "      <td>use</td>\n",
       "      <td>samsung</td>\n",
       "      <td>set</td>\n",
       "      <td>len</td>\n",
       "      <td>work</td>\n",
       "      <td>use</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>one</td>\n",
       "      <td>get</td>\n",
       "      <td>work</td>\n",
       "      <td>one</td>\n",
       "      <td>pictur</td>\n",
       "      <td>one</td>\n",
       "      <td>samsung</td>\n",
       "      <td>canon</td>\n",
       "      <td>devic</td>\n",
       "      <td>canon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>product</td>\n",
       "      <td>tv</td>\n",
       "      <td>app</td>\n",
       "      <td>product</td>\n",
       "      <td>take</td>\n",
       "      <td>look</td>\n",
       "      <td>screen</td>\n",
       "      <td>samsung</td>\n",
       "      <td>appl</td>\n",
       "      <td>one</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>tablet</td>\n",
       "      <td>camera</td>\n",
       "      <td>would</td>\n",
       "      <td>amazon</td>\n",
       "      <td>would</td>\n",
       "      <td>pictur</td>\n",
       "      <td>use</td>\n",
       "      <td>use</td>\n",
       "      <td>app</td>\n",
       "      <td>would</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ipad</td>\n",
       "      <td>bought</td>\n",
       "      <td>get</td>\n",
       "      <td>would</td>\n",
       "      <td>work</td>\n",
       "      <td>use</td>\n",
       "      <td>one</td>\n",
       "      <td>would</td>\n",
       "      <td>get</td>\n",
       "      <td>get</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>great</td>\n",
       "      <td>samsung</td>\n",
       "      <td>samsung</td>\n",
       "      <td>get</td>\n",
       "      <td>one</td>\n",
       "      <td>get</td>\n",
       "      <td>camera</td>\n",
       "      <td>get</td>\n",
       "      <td>connect</td>\n",
       "      <td>purchas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>use</td>\n",
       "      <td>back</td>\n",
       "      <td>one</td>\n",
       "      <td>purchas</td>\n",
       "      <td>great</td>\n",
       "      <td>would</td>\n",
       "      <td>would</td>\n",
       "      <td>one</td>\n",
       "      <td>one</td>\n",
       "      <td>samsung</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>purchas</td>\n",
       "      <td>screen</td>\n",
       "      <td>appl</td>\n",
       "      <td>appl</td>\n",
       "      <td>get</td>\n",
       "      <td>screen</td>\n",
       "      <td>time</td>\n",
       "      <td>amazon</td>\n",
       "      <td>samsung</td>\n",
       "      <td>len</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>appl</td>\n",
       "      <td>amazon</td>\n",
       "      <td>devic</td>\n",
       "      <td>camera</td>\n",
       "      <td>photo</td>\n",
       "      <td>great</td>\n",
       "      <td>get</td>\n",
       "      <td>return</td>\n",
       "      <td>problem</td>\n",
       "      <td>time</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Topic1  Topic10   Topic2   Topic3  Topic4   Topic5   Topic6   Topic7  \\\n",
       "0   camera      use      use      use  camera       tv       tv   camera   \n",
       "1     love      one       tv   tablet     use  samsung      set      len   \n",
       "2      one      get     work      one  pictur      one  samsung    canon   \n",
       "3  product       tv      app  product    take     look   screen  samsung   \n",
       "4   tablet   camera    would   amazon   would   pictur      use      use   \n",
       "5     ipad   bought      get    would    work      use      one    would   \n",
       "6    great  samsung  samsung      get     one      get   camera      get   \n",
       "7      use     back      one  purchas   great    would    would      one   \n",
       "8  purchas   screen     appl     appl     get   screen     time   amazon   \n",
       "9     appl   amazon    devic   camera   photo    great      get   return   \n",
       "\n",
       "    Topic8   Topic9  \n",
       "0      use   camera  \n",
       "1     work      use  \n",
       "2    devic    canon  \n",
       "3     appl      one  \n",
       "4      app    would  \n",
       "5      get      get  \n",
       "6  connect  purchas  \n",
       "7      one  samsung  \n",
       "8  samsung      len  \n",
       "9  problem     time  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samp.getTopKWordsByTopic(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 연습장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 10, 2)"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(sampler.n_wkl + sampler.beta).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "firstFactor = (sampler.n_wkl + sampler.beta) / np.expand_dims(sampler.n_kl + sampler.n_wkl.shape[0] * sampler.beta, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "firstFactor = (sampler.n_wkl + sampler.beta) / \\\n",
    "              np.expand_dims(sampler.n_kl + sampler.n_wkl.shape[0] * sampler.beta, axis=0)\n",
    "\n",
    "topic_similarity = ot.softmax(np.dot(sampler.topicVectors,\n",
    "                                     sampler.wordVectors.T))  # ( K x V)\n",
    "senti_similarity = ot.softmax(np.dot(sampler.sentimentVector,\n",
    "                                     sampler.wordVectors.T))  # ( L x V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "firstFactor /= firstFactor.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 2)"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " (sampler.n_kl + sampler.n_wkl.shape[0] * sampler.beta).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 64.1],\n",
       "       [ 78.1],\n",
       "       [ 13.1],\n",
       "       ..., \n",
       "       [  3.1],\n",
       "       [  4.1],\n",
       "       [  9.1]])"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.expand_dims(sampler.ns_d + sampler.numTopics * sampler.alpha, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "?np.expand_dims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 10, 1)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.expand_dims(sampler.ns_dk + sampler.numSentiments*sampler.gamma, axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 10, 2)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[526, 27, 2691, 1258, 125, 1063, 1492]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampler.doc_sent_word_dict[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic_1_neg</th>\n",
       "      <th>topic_1_pos</th>\n",
       "      <th>topic_2_neg</th>\n",
       "      <th>topic_2_pos</th>\n",
       "      <th>topic_3_neg</th>\n",
       "      <th>topic_3_pos</th>\n",
       "      <th>topic_4_neg</th>\n",
       "      <th>topic_4_pos</th>\n",
       "      <th>topic_5_neg</th>\n",
       "      <th>topic_5_pos</th>\n",
       "      <th>topic_6_neg</th>\n",
       "      <th>topic_6_pos</th>\n",
       "      <th>topic_7_neg</th>\n",
       "      <th>topic_7_pos</th>\n",
       "      <th>topic_8_neg</th>\n",
       "      <th>topic_8_pos</th>\n",
       "      <th>topic_9_neg</th>\n",
       "      <th>topic_9_pos</th>\n",
       "      <th>topic_10_neg</th>\n",
       "      <th>topic_10_pos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tv</td>\n",
       "      <td>supportgp</td>\n",
       "      <td>supportgp</td>\n",
       "      <td>supportgp</td>\n",
       "      <td>supportgp</td>\n",
       "      <td>supportgp</td>\n",
       "      <td>supportgp</td>\n",
       "      <td>supportgp</td>\n",
       "      <td>supportgp</td>\n",
       "      <td>supportgp</td>\n",
       "      <td>supportgp</td>\n",
       "      <td>supportgp</td>\n",
       "      <td>supportgp</td>\n",
       "      <td>supportgp</td>\n",
       "      <td>supportgp</td>\n",
       "      <td>supportgp</td>\n",
       "      <td>supportgp</td>\n",
       "      <td>supportgp</td>\n",
       "      <td>handl</td>\n",
       "      <td>supportgp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>use</td>\n",
       "      <td>hunt</td>\n",
       "      <td>hunt</td>\n",
       "      <td>hunt</td>\n",
       "      <td>hunt</td>\n",
       "      <td>hunt</td>\n",
       "      <td>hunt</td>\n",
       "      <td>hunt</td>\n",
       "      <td>hunt</td>\n",
       "      <td>hunt</td>\n",
       "      <td>hunt</td>\n",
       "      <td>hunt</td>\n",
       "      <td>hunt</td>\n",
       "      <td>hunt</td>\n",
       "      <td>hunt</td>\n",
       "      <td>hunt</td>\n",
       "      <td>hunt</td>\n",
       "      <td>hunt</td>\n",
       "      <td>mp4*</td>\n",
       "      <td>hunt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tablet</td>\n",
       "      <td>winner</td>\n",
       "      <td>winner</td>\n",
       "      <td>winner</td>\n",
       "      <td>winner</td>\n",
       "      <td>winner</td>\n",
       "      <td>winner</td>\n",
       "      <td>winner</td>\n",
       "      <td>winner</td>\n",
       "      <td>winner</td>\n",
       "      <td>winner</td>\n",
       "      <td>winner</td>\n",
       "      <td>winner</td>\n",
       "      <td>winner</td>\n",
       "      <td>winner</td>\n",
       "      <td>winner</td>\n",
       "      <td>winner</td>\n",
       "      <td>winner</td>\n",
       "      <td>wmv3</td>\n",
       "      <td>winner</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>samsung</td>\n",
       "      <td>nand</td>\n",
       "      <td>nand</td>\n",
       "      <td>nand</td>\n",
       "      <td>nand</td>\n",
       "      <td>nand</td>\n",
       "      <td>nand</td>\n",
       "      <td>nand</td>\n",
       "      <td>nand</td>\n",
       "      <td>nand</td>\n",
       "      <td>nand</td>\n",
       "      <td>nand</td>\n",
       "      <td>nand</td>\n",
       "      <td>nand</td>\n",
       "      <td>nand</td>\n",
       "      <td>nand</td>\n",
       "      <td>nand</td>\n",
       "      <td>nand</td>\n",
       "      <td>window</td>\n",
       "      <td>nand</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>app</td>\n",
       "      <td>justifi</td>\n",
       "      <td>justifi</td>\n",
       "      <td>justifi</td>\n",
       "      <td>justifi</td>\n",
       "      <td>justifi</td>\n",
       "      <td>justifi</td>\n",
       "      <td>justifi</td>\n",
       "      <td>justifi</td>\n",
       "      <td>justifi</td>\n",
       "      <td>justifi</td>\n",
       "      <td>justifi</td>\n",
       "      <td>justifi</td>\n",
       "      <td>justifi</td>\n",
       "      <td>justifi</td>\n",
       "      <td>justifi</td>\n",
       "      <td>justifi</td>\n",
       "      <td>justifi</td>\n",
       "      <td>current</td>\n",
       "      <td>justifi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>one</td>\n",
       "      <td>univers_remot</td>\n",
       "      <td>univers_remot</td>\n",
       "      <td>univers_remot</td>\n",
       "      <td>univers_remot</td>\n",
       "      <td>univers_remot</td>\n",
       "      <td>univers_remot</td>\n",
       "      <td>univers_remot</td>\n",
       "      <td>univers_remot</td>\n",
       "      <td>univers_remot</td>\n",
       "      <td>univers_remot</td>\n",
       "      <td>univers_remot</td>\n",
       "      <td>univers_remot</td>\n",
       "      <td>univers_remot</td>\n",
       "      <td>univers_remot</td>\n",
       "      <td>univers_remot</td>\n",
       "      <td>univers_remot</td>\n",
       "      <td>univers_remot</td>\n",
       "      <td>thing</td>\n",
       "      <td>univers_remot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>veri</td>\n",
       "      <td>ir_blaster</td>\n",
       "      <td>ir_blaster</td>\n",
       "      <td>ir_blaster</td>\n",
       "      <td>ir_blaster</td>\n",
       "      <td>ir_blaster</td>\n",
       "      <td>ir_blaster</td>\n",
       "      <td>ir_blaster</td>\n",
       "      <td>ir_blaster</td>\n",
       "      <td>ir_blaster</td>\n",
       "      <td>ir_blaster</td>\n",
       "      <td>ir_blaster</td>\n",
       "      <td>ir_blaster</td>\n",
       "      <td>ir_blaster</td>\n",
       "      <td>ir_blaster</td>\n",
       "      <td>ir_blaster</td>\n",
       "      <td>ir_blaster</td>\n",
       "      <td>ir_blaster</td>\n",
       "      <td>not_found</td>\n",
       "      <td>ir_blaster</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>like</td>\n",
       "      <td>companion</td>\n",
       "      <td>companion</td>\n",
       "      <td>companion</td>\n",
       "      <td>companion</td>\n",
       "      <td>companion</td>\n",
       "      <td>companion</td>\n",
       "      <td>companion</td>\n",
       "      <td>companion</td>\n",
       "      <td>companion</td>\n",
       "      <td>companion</td>\n",
       "      <td>companion</td>\n",
       "      <td>companion</td>\n",
       "      <td>companion</td>\n",
       "      <td>companion</td>\n",
       "      <td>companion</td>\n",
       "      <td>companion</td>\n",
       "      <td>companion</td>\n",
       "      <td>avi*</td>\n",
       "      <td>companion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>set</td>\n",
       "      <td>z</td>\n",
       "      <td>z</td>\n",
       "      <td>z</td>\n",
       "      <td>z</td>\n",
       "      <td>z</td>\n",
       "      <td>z</td>\n",
       "      <td>z</td>\n",
       "      <td>z</td>\n",
       "      <td>z</td>\n",
       "      <td>z</td>\n",
       "      <td>z</td>\n",
       "      <td>z</td>\n",
       "      <td>z</td>\n",
       "      <td>z</td>\n",
       "      <td>z</td>\n",
       "      <td>z</td>\n",
       "      <td>z</td>\n",
       "      <td>asf*</td>\n",
       "      <td>z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>get</td>\n",
       "      <td>student</td>\n",
       "      <td>student</td>\n",
       "      <td>student</td>\n",
       "      <td>student</td>\n",
       "      <td>student</td>\n",
       "      <td>student</td>\n",
       "      <td>student</td>\n",
       "      <td>student</td>\n",
       "      <td>student</td>\n",
       "      <td>student</td>\n",
       "      <td>student</td>\n",
       "      <td>student</td>\n",
       "      <td>student</td>\n",
       "      <td>student</td>\n",
       "      <td>student</td>\n",
       "      <td>student</td>\n",
       "      <td>student</td>\n",
       "      <td>year_ago</td>\n",
       "      <td>student</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  topic_1_neg    topic_1_pos    topic_2_neg    topic_2_pos    topic_3_neg  \\\n",
       "0          tv      supportgp      supportgp      supportgp      supportgp   \n",
       "1         use           hunt           hunt           hunt           hunt   \n",
       "2      tablet         winner         winner         winner         winner   \n",
       "3     samsung           nand           nand           nand           nand   \n",
       "4         app        justifi        justifi        justifi        justifi   \n",
       "5         one  univers_remot  univers_remot  univers_remot  univers_remot   \n",
       "6        veri     ir_blaster     ir_blaster     ir_blaster     ir_blaster   \n",
       "7        like      companion      companion      companion      companion   \n",
       "8         set              z              z              z              z   \n",
       "9         get        student        student        student        student   \n",
       "\n",
       "     topic_3_pos    topic_4_neg    topic_4_pos    topic_5_neg    topic_5_pos  \\\n",
       "0      supportgp      supportgp      supportgp      supportgp      supportgp   \n",
       "1           hunt           hunt           hunt           hunt           hunt   \n",
       "2         winner         winner         winner         winner         winner   \n",
       "3           nand           nand           nand           nand           nand   \n",
       "4        justifi        justifi        justifi        justifi        justifi   \n",
       "5  univers_remot  univers_remot  univers_remot  univers_remot  univers_remot   \n",
       "6     ir_blaster     ir_blaster     ir_blaster     ir_blaster     ir_blaster   \n",
       "7      companion      companion      companion      companion      companion   \n",
       "8              z              z              z              z              z   \n",
       "9        student        student        student        student        student   \n",
       "\n",
       "     topic_6_neg    topic_6_pos    topic_7_neg    topic_7_pos    topic_8_neg  \\\n",
       "0      supportgp      supportgp      supportgp      supportgp      supportgp   \n",
       "1           hunt           hunt           hunt           hunt           hunt   \n",
       "2         winner         winner         winner         winner         winner   \n",
       "3           nand           nand           nand           nand           nand   \n",
       "4        justifi        justifi        justifi        justifi        justifi   \n",
       "5  univers_remot  univers_remot  univers_remot  univers_remot  univers_remot   \n",
       "6     ir_blaster     ir_blaster     ir_blaster     ir_blaster     ir_blaster   \n",
       "7      companion      companion      companion      companion      companion   \n",
       "8              z              z              z              z              z   \n",
       "9        student        student        student        student        student   \n",
       "\n",
       "     topic_8_pos    topic_9_neg    topic_9_pos topic_10_neg   topic_10_pos  \n",
       "0      supportgp      supportgp      supportgp        handl      supportgp  \n",
       "1           hunt           hunt           hunt         mp4*           hunt  \n",
       "2         winner         winner         winner         wmv3         winner  \n",
       "3           nand           nand           nand       window           nand  \n",
       "4        justifi        justifi        justifi      current        justifi  \n",
       "5  univers_remot  univers_remot  univers_remot        thing  univers_remot  \n",
       "6     ir_blaster     ir_blaster     ir_blaster    not_found     ir_blaster  \n",
       "7      companion      companion      companion         avi*      companion  \n",
       "8              z              z              z         asf*              z  \n",
       "9        student        student        student     year_ago        student  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampler.getTopKWords(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1795 47\n",
      "1825 18\n",
      "1837 6\n",
      "1840 3\n",
      "1841 2\n",
      "1843 0\n",
      "2076 31\n",
      "2092 15\n",
      "2101 6\n",
      "2102 5\n",
      "2104 3\n",
      "2105 2\n",
      "2107 0\n"
     ]
    }
   ],
   "source": [
    "for d in range(sampler.numDocs):\n",
    "    for m in range(sampler.numSentence[d]):\n",
    "        try:\n",
    "            topic_similarity = ot.softmax(np.dot(sampler.topicVectors,\n",
    "                                      sampler.wordVectors[sampler.doc_sent_word_dict[d][m]].T)) #( K x num words in sentence)\n",
    "            senti_similarity = ot.softmax(np.dot(sampler.sentimentVector,\n",
    "                                      sampler.wordVectors[sampler.doc_sent_word_dict[d][m]].T)) #( L x num words in sentence)\n",
    "            vector_similarity = ot.softmax(np.dot(topic_similarity, senti_similarity.T))\n",
    "        except:\n",
    "            print(d,m)\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[],\n",
       " [0, 903, 113],\n",
       " [2545, 1529, 98, 529, 165, 3346],\n",
       " [153,\n",
       "  2424,\n",
       "  91,\n",
       "  2424,\n",
       "  4813,\n",
       "  79,\n",
       "  251,\n",
       "  449,\n",
       "  400,\n",
       "  135,\n",
       "  0,\n",
       "  264,\n",
       "  825,\n",
       "  316,\n",
       "  2274,\n",
       "  3083,\n",
       "  3017,\n",
       "  3460,\n",
       "  170,\n",
       "  14,\n",
       "  1,\n",
       "  1118,\n",
       "  92,\n",
       "  1268,\n",
       "  507],\n",
       " [929, 1, 27, 2672, 46, 143, 63, 48, 709, 304],\n",
       " [4962, 65, 314, 285, 314, 2421],\n",
       " [14, 2672, 279, 225, 1253, 4848, 41, 92],\n",
       " [1903, 1124, 1253],\n",
       " [104, 604, 9, 317, 249, 2672, 45, 1118],\n",
       " [1253],\n",
       " [759],\n",
       " [3108],\n",
       " [1502, 249, 489, 27, 249, 84, 8, 223, 111],\n",
       " [2466, 0, 136, 4],\n",
       " [3],\n",
       " [27],\n",
       " [1542, 136, 4, 13, 567, 98, 414, 903, 313, 13, 23, 45, 8, 307]]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampler.doc_sent_word_dict[2107]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 100)"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampler.topicVectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2894, 100)"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampler.wordVectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 1063]"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampler.doc_sent_word_dict[0][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "probabilities_ts = sampler.conditionalDistribution(0,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 해당 문서와 topic vector와 유사도\n",
    "topic_similarity = ot.softmax(np.dot(sampler.topicVectors, sampler.wordVectors[sampler.doc_sent_word_dict[0][1]].T))\n",
    "#topic_similarity /= topic_similarity.sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 7)"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_similarity.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "senti_similarity = ot.softmax(np.dot(sampler.sentimentVector, sampler.wordVectors[sampler.doc_sent_word_dict[0][1]].T))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 7)"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "senti_similarity.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  2.02158501e-03,   1.93985192e-03,   5.31407746e-04,\n",
       "          1.05096038e-03,   5.05358453e-03,   1.61914991e-03,\n",
       "          3.83634056e-03],\n",
       "       [  3.43673315e-04,   2.43161809e-02,   9.56667229e-01,\n",
       "          1.71550308e-04,   9.82957866e-04,   3.72137117e-04,\n",
       "          1.09339132e-03]])"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "senti_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vector_similarity = ot.softmax(np.dot(topic_similarity, senti_similarity.T))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "result = probabilities_ts * vector_similarity\n",
    "result/= result.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.flatten().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ind = sampleFromCategorical(result.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 1)"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unravel_index(ind, result.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic_1_neg</th>\n",
       "      <th>topic_1_pos</th>\n",
       "      <th>topic_2_neg</th>\n",
       "      <th>topic_2_pos</th>\n",
       "      <th>topic_3_neg</th>\n",
       "      <th>topic_3_pos</th>\n",
       "      <th>topic_4_neg</th>\n",
       "      <th>topic_4_pos</th>\n",
       "      <th>topic_5_neg</th>\n",
       "      <th>topic_5_pos</th>\n",
       "      <th>topic_6_neg</th>\n",
       "      <th>topic_6_pos</th>\n",
       "      <th>topic_7_neg</th>\n",
       "      <th>topic_7_pos</th>\n",
       "      <th>topic_8_neg</th>\n",
       "      <th>topic_8_pos</th>\n",
       "      <th>topic_9_neg</th>\n",
       "      <th>topic_9_pos</th>\n",
       "      <th>topic_10_neg</th>\n",
       "      <th>topic_10_pos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tablet</td>\n",
       "      <td>tablet</td>\n",
       "      <td>use</td>\n",
       "      <td>tv</td>\n",
       "      <td>tv</td>\n",
       "      <td>use</td>\n",
       "      <td>tv</td>\n",
       "      <td>tv</td>\n",
       "      <td>tv</td>\n",
       "      <td>tv</td>\n",
       "      <td>tv</td>\n",
       "      <td>tv</td>\n",
       "      <td>tv</td>\n",
       "      <td>tablet</td>\n",
       "      <td>tv</td>\n",
       "      <td>tv</td>\n",
       "      <td>tv</td>\n",
       "      <td>use</td>\n",
       "      <td>tv</td>\n",
       "      <td>use</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>use</td>\n",
       "      <td>tv</td>\n",
       "      <td>tv</td>\n",
       "      <td>use</td>\n",
       "      <td>use</td>\n",
       "      <td>tablet</td>\n",
       "      <td>use</td>\n",
       "      <td>use</td>\n",
       "      <td>use</td>\n",
       "      <td>use</td>\n",
       "      <td>use</td>\n",
       "      <td>samsung</td>\n",
       "      <td>tablet</td>\n",
       "      <td>tv</td>\n",
       "      <td>use</td>\n",
       "      <td>use</td>\n",
       "      <td>tablet</td>\n",
       "      <td>tv</td>\n",
       "      <td>samsung</td>\n",
       "      <td>tv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tv</td>\n",
       "      <td>use</td>\n",
       "      <td>samsung</td>\n",
       "      <td>tablet</td>\n",
       "      <td>tablet</td>\n",
       "      <td>tv</td>\n",
       "      <td>samsung</td>\n",
       "      <td>samsung</td>\n",
       "      <td>samsung</td>\n",
       "      <td>samsung</td>\n",
       "      <td>samsung</td>\n",
       "      <td>use</td>\n",
       "      <td>use</td>\n",
       "      <td>use</td>\n",
       "      <td>samsung</td>\n",
       "      <td>samsung</td>\n",
       "      <td>use</td>\n",
       "      <td>tablet</td>\n",
       "      <td>use</td>\n",
       "      <td>samsung</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>samsung</td>\n",
       "      <td>samsung</td>\n",
       "      <td>tablet</td>\n",
       "      <td>samsung</td>\n",
       "      <td>samsung</td>\n",
       "      <td>samsung</td>\n",
       "      <td>tablet</td>\n",
       "      <td>tablet</td>\n",
       "      <td>tablet</td>\n",
       "      <td>tablet</td>\n",
       "      <td>tablet</td>\n",
       "      <td>tablet</td>\n",
       "      <td>samsung</td>\n",
       "      <td>samsung</td>\n",
       "      <td>tablet</td>\n",
       "      <td>tablet</td>\n",
       "      <td>samsung</td>\n",
       "      <td>samsung</td>\n",
       "      <td>tablet</td>\n",
       "      <td>tablet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>app</td>\n",
       "      <td>app</td>\n",
       "      <td>app</td>\n",
       "      <td>app</td>\n",
       "      <td>one</td>\n",
       "      <td>app</td>\n",
       "      <td>one</td>\n",
       "      <td>app</td>\n",
       "      <td>app</td>\n",
       "      <td>app</td>\n",
       "      <td>veri</td>\n",
       "      <td>veri</td>\n",
       "      <td>app</td>\n",
       "      <td>app</td>\n",
       "      <td>app</td>\n",
       "      <td>app</td>\n",
       "      <td>app</td>\n",
       "      <td>app</td>\n",
       "      <td>one</td>\n",
       "      <td>one</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>veri</td>\n",
       "      <td>veri</td>\n",
       "      <td>one</td>\n",
       "      <td>veri</td>\n",
       "      <td>veri</td>\n",
       "      <td>veri</td>\n",
       "      <td>veri</td>\n",
       "      <td>one</td>\n",
       "      <td>one</td>\n",
       "      <td>veri</td>\n",
       "      <td>one</td>\n",
       "      <td>app</td>\n",
       "      <td>one</td>\n",
       "      <td>one</td>\n",
       "      <td>veri</td>\n",
       "      <td>one</td>\n",
       "      <td>one</td>\n",
       "      <td>one</td>\n",
       "      <td>veri</td>\n",
       "      <td>app</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>one</td>\n",
       "      <td>one</td>\n",
       "      <td>veri</td>\n",
       "      <td>like</td>\n",
       "      <td>app</td>\n",
       "      <td>one</td>\n",
       "      <td>app</td>\n",
       "      <td>veri</td>\n",
       "      <td>veri</td>\n",
       "      <td>one</td>\n",
       "      <td>set</td>\n",
       "      <td>one</td>\n",
       "      <td>veri</td>\n",
       "      <td>veri</td>\n",
       "      <td>one</td>\n",
       "      <td>set</td>\n",
       "      <td>veri</td>\n",
       "      <td>like</td>\n",
       "      <td>app</td>\n",
       "      <td>veri</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>set</td>\n",
       "      <td>tab</td>\n",
       "      <td>like</td>\n",
       "      <td>one</td>\n",
       "      <td>like</td>\n",
       "      <td>camera</td>\n",
       "      <td>camera</td>\n",
       "      <td>set</td>\n",
       "      <td>like</td>\n",
       "      <td>set</td>\n",
       "      <td>like</td>\n",
       "      <td>set</td>\n",
       "      <td>like</td>\n",
       "      <td>get</td>\n",
       "      <td>set</td>\n",
       "      <td>veri</td>\n",
       "      <td>like</td>\n",
       "      <td>tab</td>\n",
       "      <td>like</td>\n",
       "      <td>like</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>look</td>\n",
       "      <td>screen</td>\n",
       "      <td>get</td>\n",
       "      <td>set</td>\n",
       "      <td>set</td>\n",
       "      <td>like</td>\n",
       "      <td>like</td>\n",
       "      <td>like</td>\n",
       "      <td>set</td>\n",
       "      <td>get</td>\n",
       "      <td>app</td>\n",
       "      <td>camera</td>\n",
       "      <td>video</td>\n",
       "      <td>like</td>\n",
       "      <td>tab</td>\n",
       "      <td>like</td>\n",
       "      <td>set</td>\n",
       "      <td>veri</td>\n",
       "      <td>get</td>\n",
       "      <td>also</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>get</td>\n",
       "      <td>like</td>\n",
       "      <td>look</td>\n",
       "      <td>screen</td>\n",
       "      <td>video</td>\n",
       "      <td>set</td>\n",
       "      <td>get</td>\n",
       "      <td>look</td>\n",
       "      <td>get</td>\n",
       "      <td>like</td>\n",
       "      <td>look</td>\n",
       "      <td>like</td>\n",
       "      <td>screen</td>\n",
       "      <td>set</td>\n",
       "      <td>get</td>\n",
       "      <td>look</td>\n",
       "      <td>get</td>\n",
       "      <td>get</td>\n",
       "      <td>also</td>\n",
       "      <td>look</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  topic_1_neg topic_1_pos topic_2_neg topic_2_pos topic_3_neg topic_3_pos  \\\n",
       "0      tablet      tablet         use          tv          tv         use   \n",
       "1         use          tv          tv         use         use      tablet   \n",
       "2          tv         use     samsung      tablet      tablet          tv   \n",
       "3     samsung     samsung      tablet     samsung     samsung     samsung   \n",
       "4         app         app         app         app         one         app   \n",
       "5        veri        veri         one        veri        veri        veri   \n",
       "6         one         one        veri        like         app         one   \n",
       "7         set         tab        like         one        like      camera   \n",
       "8        look      screen         get         set         set        like   \n",
       "9         get        like        look      screen       video         set   \n",
       "\n",
       "  topic_4_neg topic_4_pos topic_5_neg topic_5_pos topic_6_neg topic_6_pos  \\\n",
       "0          tv          tv          tv          tv          tv          tv   \n",
       "1         use         use         use         use         use     samsung   \n",
       "2     samsung     samsung     samsung     samsung     samsung         use   \n",
       "3      tablet      tablet      tablet      tablet      tablet      tablet   \n",
       "4         one         app         app         app        veri        veri   \n",
       "5        veri         one         one        veri         one         app   \n",
       "6         app        veri        veri         one         set         one   \n",
       "7      camera         set        like         set        like         set   \n",
       "8        like        like         set         get         app      camera   \n",
       "9         get        look         get        like        look        like   \n",
       "\n",
       "  topic_7_neg topic_7_pos topic_8_neg topic_8_pos topic_9_neg topic_9_pos  \\\n",
       "0          tv      tablet          tv          tv          tv         use   \n",
       "1      tablet          tv         use         use      tablet          tv   \n",
       "2         use         use     samsung     samsung         use      tablet   \n",
       "3     samsung     samsung      tablet      tablet     samsung     samsung   \n",
       "4         app         app         app         app         app         app   \n",
       "5         one         one        veri         one         one         one   \n",
       "6        veri        veri         one         set        veri        like   \n",
       "7        like         get         set        veri        like         tab   \n",
       "8       video        like         tab        like         set        veri   \n",
       "9      screen         set         get        look         get         get   \n",
       "\n",
       "  topic_10_neg topic_10_pos  \n",
       "0           tv          use  \n",
       "1      samsung           tv  \n",
       "2          use      samsung  \n",
       "3       tablet       tablet  \n",
       "4          one          one  \n",
       "5         veri          app  \n",
       "6          app         veri  \n",
       "7         like         like  \n",
       "8          get         also  \n",
       "9         also         look  "
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampler.getTopKWords(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pos_score = np.dot(sampler.sentimentVector, sampler.wordVectors[sampler.doc_sent_word_dict[0][1]].T).sum(axis=1)\n",
    "pos_score = ot.softmax(pos_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  1.34681396e-04,   9.99865319e-01])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(pos_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic_1_neg</th>\n",
       "      <th>topic_1_pos</th>\n",
       "      <th>topic_2_neg</th>\n",
       "      <th>topic_2_pos</th>\n",
       "      <th>topic_3_neg</th>\n",
       "      <th>topic_3_pos</th>\n",
       "      <th>topic_4_neg</th>\n",
       "      <th>topic_4_pos</th>\n",
       "      <th>topic_5_neg</th>\n",
       "      <th>topic_5_pos</th>\n",
       "      <th>topic_6_neg</th>\n",
       "      <th>topic_6_pos</th>\n",
       "      <th>topic_7_neg</th>\n",
       "      <th>topic_7_pos</th>\n",
       "      <th>topic_8_neg</th>\n",
       "      <th>topic_8_pos</th>\n",
       "      <th>topic_9_neg</th>\n",
       "      <th>topic_9_pos</th>\n",
       "      <th>topic_10_neg</th>\n",
       "      <th>topic_10_pos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>exceed_expect</td>\n",
       "      <td>exceed_expect</td>\n",
       "      <td>exceed_expect</td>\n",
       "      <td>exceed_expect</td>\n",
       "      <td>audio</td>\n",
       "      <td>tv</td>\n",
       "      <td>exceed_expect</td>\n",
       "      <td>exceed_expect</td>\n",
       "      <td>exceed_expect</td>\n",
       "      <td>hdcp</td>\n",
       "      <td>exceed_expect</td>\n",
       "      <td>exceed_expect</td>\n",
       "      <td>exceed_expect</td>\n",
       "      <td>exceed_expect</td>\n",
       "      <td>exceed_expect</td>\n",
       "      <td>lpcm</td>\n",
       "      <td>exceed_expect</td>\n",
       "      <td>exceed_expect</td>\n",
       "      <td>samsung</td>\n",
       "      <td>exceed_expect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>packaging.galaxi</td>\n",
       "      <td>packaging.galaxi</td>\n",
       "      <td>packaging.galaxi</td>\n",
       "      <td>packaging.galaxi</td>\n",
       "      <td>support</td>\n",
       "      <td>use</td>\n",
       "      <td>packaging.galaxi</td>\n",
       "      <td>packaging.galaxi</td>\n",
       "      <td>packaging.galaxi</td>\n",
       "      <td>5.2-channel</td>\n",
       "      <td>packaging.galaxi</td>\n",
       "      <td>packaging.galaxi</td>\n",
       "      <td>packaging.galaxi</td>\n",
       "      <td>packaging.galaxi</td>\n",
       "      <td>packaging.galaxi</td>\n",
       "      <td>digit</td>\n",
       "      <td>packaging.galaxi</td>\n",
       "      <td>packaging.galaxi</td>\n",
       "      <td>wqxga_ppi</td>\n",
       "      <td>packaging.galaxi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>satur</td>\n",
       "      <td>satur</td>\n",
       "      <td>satur</td>\n",
       "      <td>satur</td>\n",
       "      <td>rington</td>\n",
       "      <td>tablet</td>\n",
       "      <td>satur</td>\n",
       "      <td>satur</td>\n",
       "      <td>satur</td>\n",
       "      <td>tx-nr636</td>\n",
       "      <td>satur</td>\n",
       "      <td>satur</td>\n",
       "      <td>satur</td>\n",
       "      <td>satur</td>\n",
       "      <td>satur</td>\n",
       "      <td>bd-f5900</td>\n",
       "      <td>satur</td>\n",
       "      <td>satur</td>\n",
       "      <td>exyno_octa</td>\n",
       "      <td>satur</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ultrahd</td>\n",
       "      <td>ultrahd</td>\n",
       "      <td>ultrahd</td>\n",
       "      <td>ultrahd</td>\n",
       "      <td>tone</td>\n",
       "      <td>samsung</td>\n",
       "      <td>ultrahd</td>\n",
       "      <td>ultrahd</td>\n",
       "      <td>ultrahd</td>\n",
       "      <td>featur</td>\n",
       "      <td>ultrahd</td>\n",
       "      <td>ultrahd</td>\n",
       "      <td>ultrahd</td>\n",
       "      <td>ultrahd</td>\n",
       "      <td>ultrahd</td>\n",
       "      <td>aac</td>\n",
       "      <td>ultrahd</td>\n",
       "      <td>ultrahd</td>\n",
       "      <td>quadcor_gb</td>\n",
       "      <td>ultrahd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>not_keep</td>\n",
       "      <td>not_keep</td>\n",
       "      <td>not_keep</td>\n",
       "      <td>not_keep</td>\n",
       "      <td>mp3</td>\n",
       "      <td>app</td>\n",
       "      <td>not_keep</td>\n",
       "      <td>not_keep</td>\n",
       "      <td>not_keep</td>\n",
       "      <td>onkyo</td>\n",
       "      <td>not_keep</td>\n",
       "      <td>not_keep</td>\n",
       "      <td>not_keep</td>\n",
       "      <td>not_keep</td>\n",
       "      <td>not_keep</td>\n",
       "      <td>he-aac</td>\n",
       "      <td>not_keep</td>\n",
       "      <td>not_keep</td>\n",
       "      <td>x_microusb</td>\n",
       "      <td>not_keep</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>matt</td>\n",
       "      <td>matt</td>\n",
       "      <td>matt</td>\n",
       "      <td>matt</td>\n",
       "      <td>polyphon</td>\n",
       "      <td>veri</td>\n",
       "      <td>matt</td>\n",
       "      <td>matt</td>\n",
       "      <td>matt</td>\n",
       "      <td>hdmi</td>\n",
       "      <td>matt</td>\n",
       "      <td>matt</td>\n",
       "      <td>matt</td>\n",
       "      <td>matt</td>\n",
       "      <td>matt</td>\n",
       "      <td>dolbi_digit</td>\n",
       "      <td>matt</td>\n",
       "      <td>matt</td>\n",
       "      <td>quadcor_ghz</td>\n",
       "      <td>matt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>microusb_port</td>\n",
       "      <td>microusb_port</td>\n",
       "      <td>microusb_port</td>\n",
       "      <td>microusb_port</td>\n",
       "      <td>amr</td>\n",
       "      <td>one</td>\n",
       "      <td>microusb_port</td>\n",
       "      <td>microusb_port</td>\n",
       "      <td>microusb_port</td>\n",
       "      <td>tx-nr535</td>\n",
       "      <td>microusb_port</td>\n",
       "      <td>microusb_port</td>\n",
       "      <td>microusb_port</td>\n",
       "      <td>microusb_port</td>\n",
       "      <td>microusb_port</td>\n",
       "      <td>videos.so</td>\n",
       "      <td>microusb_port</td>\n",
       "      <td>microusb_port</td>\n",
       "      <td>ram_gb</td>\n",
       "      <td>microusb_port</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>vast</td>\n",
       "      <td>vast</td>\n",
       "      <td>vast</td>\n",
       "      <td>vast</td>\n",
       "      <td>aac</td>\n",
       "      <td>like</td>\n",
       "      <td>vast</td>\n",
       "      <td>vast</td>\n",
       "      <td>vast</td>\n",
       "      <td>exceed_expect</td>\n",
       "      <td>vast</td>\n",
       "      <td>vast</td>\n",
       "      <td>vast</td>\n",
       "      <td>vast</td>\n",
       "      <td>vast</td>\n",
       "      <td>mp3/mpeg</td>\n",
       "      <td>vast</td>\n",
       "      <td>vast</td>\n",
       "      <td>kitkat_x</td>\n",
       "      <td>vast</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>not_but</td>\n",
       "      <td>not_but</td>\n",
       "      <td>not_but</td>\n",
       "      <td>not_but</td>\n",
       "      <td>wma/asf</td>\n",
       "      <td>get</td>\n",
       "      <td>not_but</td>\n",
       "      <td>not_but</td>\n",
       "      <td>not_but</td>\n",
       "      <td>microusb_port</td>\n",
       "      <td>not_but</td>\n",
       "      <td>not_but</td>\n",
       "      <td>not_but</td>\n",
       "      <td>not_but</td>\n",
       "      <td>not_but</td>\n",
       "      <td>wma</td>\n",
       "      <td>not_but</td>\n",
       "      <td>not_but</td>\n",
       "      <td>b_w</td>\n",
       "      <td>not_but</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>damag</td>\n",
       "      <td>damag</td>\n",
       "      <td>damag</td>\n",
       "      <td>damag</td>\n",
       "      <td>ogg</td>\n",
       "      <td>set</td>\n",
       "      <td>damag</td>\n",
       "      <td>damag</td>\n",
       "      <td>damag</td>\n",
       "      <td>not_but</td>\n",
       "      <td>damag</td>\n",
       "      <td>damag</td>\n",
       "      <td>damag</td>\n",
       "      <td>damag</td>\n",
       "      <td>damag</td>\n",
       "      <td>butt</td>\n",
       "      <td>damag</td>\n",
       "      <td>damag</td>\n",
       "      <td>galaxi_tabpro</td>\n",
       "      <td>damag</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        topic_1_neg       topic_1_pos       topic_2_neg       topic_2_pos  \\\n",
       "0     exceed_expect     exceed_expect     exceed_expect     exceed_expect   \n",
       "1  packaging.galaxi  packaging.galaxi  packaging.galaxi  packaging.galaxi   \n",
       "2             satur             satur             satur             satur   \n",
       "3           ultrahd           ultrahd           ultrahd           ultrahd   \n",
       "4          not_keep          not_keep          not_keep          not_keep   \n",
       "5              matt              matt              matt              matt   \n",
       "6     microusb_port     microusb_port     microusb_port     microusb_port   \n",
       "7              vast              vast              vast              vast   \n",
       "8           not_but           not_but           not_but           not_but   \n",
       "9             damag             damag             damag             damag   \n",
       "\n",
       "  topic_3_neg topic_3_pos       topic_4_neg       topic_4_pos  \\\n",
       "0       audio          tv     exceed_expect     exceed_expect   \n",
       "1     support         use  packaging.galaxi  packaging.galaxi   \n",
       "2     rington      tablet             satur             satur   \n",
       "3        tone     samsung           ultrahd           ultrahd   \n",
       "4         mp3         app          not_keep          not_keep   \n",
       "5    polyphon        veri              matt              matt   \n",
       "6         amr         one     microusb_port     microusb_port   \n",
       "7         aac        like              vast              vast   \n",
       "8     wma/asf         get           not_but           not_but   \n",
       "9         ogg         set             damag             damag   \n",
       "\n",
       "        topic_5_neg    topic_5_pos       topic_6_neg       topic_6_pos  \\\n",
       "0     exceed_expect           hdcp     exceed_expect     exceed_expect   \n",
       "1  packaging.galaxi    5.2-channel  packaging.galaxi  packaging.galaxi   \n",
       "2             satur       tx-nr636             satur             satur   \n",
       "3           ultrahd         featur           ultrahd           ultrahd   \n",
       "4          not_keep          onkyo          not_keep          not_keep   \n",
       "5              matt           hdmi              matt              matt   \n",
       "6     microusb_port       tx-nr535     microusb_port     microusb_port   \n",
       "7              vast  exceed_expect              vast              vast   \n",
       "8           not_but  microusb_port           not_but           not_but   \n",
       "9             damag        not_but             damag             damag   \n",
       "\n",
       "        topic_7_neg       topic_7_pos       topic_8_neg  topic_8_pos  \\\n",
       "0     exceed_expect     exceed_expect     exceed_expect         lpcm   \n",
       "1  packaging.galaxi  packaging.galaxi  packaging.galaxi        digit   \n",
       "2             satur             satur             satur     bd-f5900   \n",
       "3           ultrahd           ultrahd           ultrahd          aac   \n",
       "4          not_keep          not_keep          not_keep       he-aac   \n",
       "5              matt              matt              matt  dolbi_digit   \n",
       "6     microusb_port     microusb_port     microusb_port    videos.so   \n",
       "7              vast              vast              vast     mp3/mpeg   \n",
       "8           not_but           not_but           not_but          wma   \n",
       "9             damag             damag             damag         butt   \n",
       "\n",
       "        topic_9_neg       topic_9_pos   topic_10_neg      topic_10_pos  \n",
       "0     exceed_expect     exceed_expect        samsung     exceed_expect  \n",
       "1  packaging.galaxi  packaging.galaxi      wqxga_ppi  packaging.galaxi  \n",
       "2             satur             satur     exyno_octa             satur  \n",
       "3           ultrahd           ultrahd     quadcor_gb           ultrahd  \n",
       "4          not_keep          not_keep     x_microusb          not_keep  \n",
       "5              matt              matt    quadcor_ghz              matt  \n",
       "6     microusb_port     microusb_port         ram_gb     microusb_port  \n",
       "7              vast              vast       kitkat_x              vast  \n",
       "8           not_but           not_but            b_w           not_but  \n",
       "9             damag             damag  galaxi_tabpro             damag  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampler.getTopKWords(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  1.83506814e-10,   2.23290875e-04],\n",
       "       [  1.60974086e-10,   5.36969161e-05],\n",
       "       [  1.68805741e-10,   4.89018857e-05],\n",
       "       [  2.66052459e-08,   9.99230810e-01],\n",
       "       [  1.57084661e-10,   5.55868529e-05],\n",
       "       [  2.31037422e-10,   1.60966821e-04],\n",
       "       [  1.43182080e-10,   8.30383198e-05],\n",
       "       [  1.88503172e-10,   7.10861515e-05],\n",
       "       [  5.51199076e-11,   3.13752639e-05],\n",
       "       [  2.24208768e-10,   4.12188409e-05]])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampler.conditionalDistribution(0,0,20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "w = 0\n",
    "t = 1\n",
    "s = 0\n",
    "k = (sampler.n_wkl[w,t,s] + sampler.beta) / (sampler.n_kl + sampler.n_wkl.shape[0] * sampler.beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 2)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "prob = 1\n",
    "d = 10\n",
    "m = 2\n",
    "for word_idx in sampler.doc_sent_word_dict[d][m]:\n",
    "    for i in range(sampler.wordCountSentence[d][m][word_idx]):\n",
    "        prob *= sampler.n_wkl[word_idx,:,:] + sampler.beta + sampler.wordCountSentence[d][m][word_idx] -1 -i\n",
    "        prob /= prob.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  1.28128710e-11,   3.32387162e-02],\n",
       "       [  3.56568843e-28,   7.88681872e-05],\n",
       "       [  4.99174088e-23,   1.62427575e-10],\n",
       "       [  1.12064780e-23,   2.05496915e-04],\n",
       "       [  4.62029047e-24,   2.66786082e-06],\n",
       "       [  2.00164567e-14,   9.65114191e-01],\n",
       "       [  1.34288983e-24,   1.35861253e-03],\n",
       "       [  4.41120245e-16,   1.36397954e-06],\n",
       "       [  3.06202612e-36,   1.09052640e-16],\n",
       "       [  3.60526903e-34,   8.29690294e-08]])"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 270.01,  995.01],\n",
       "       [  32.01,  390.01],\n",
       "       [  60.01,  359.01],\n",
       "       [  67.01,  449.01],\n",
       "       [  82.01,  480.01],\n",
       "       [ 167.01,  679.01],\n",
       "       [  74.01,  525.01],\n",
       "       [ 148.01,  535.01],\n",
       "       [  10.01,  148.01],\n",
       "       [  29.01,  286.01]])"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampler.n_wkl[word_idx,:,:] + sampler.beta + sampler.wordCountSentence[d][m][word_idx] -1 -i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampler.wordCountSentence[d][m][word_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "topic_score = np.dot(sampler.topicVectors, sampler.wordVectors[sampler.doc_sent_word_dict[0][1]].T).sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampleFromCategorical(ot.softmax(topic_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[526, 27, 2691, 1258, 125, 1063, 1492]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampler.doc_sent_word_dict[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({27: 1, 125: 1, 526: 1, 1063: 1, 1258: 1, 1492: 1, 2691: 1})"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampler.wordCountSentence[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d = 0\n",
    "m = 1\n",
    "\n",
    "prob = 1\n",
    "for word_idx in sampler.doc_sent_word_dict[d][m]:\n",
    "    for i in range(sampler.wordCountSentence[d][m][word_idx]):\n",
    "        prob *= sampler.n_wkl[word_idx, :, :] + sampler.beta + sampler.wordCountSentence[d][m][word_idx] - 1 - i\n",
    "        prob /= prob.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[526, 27, 2691, 1258, 125, 1063, 1492]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampler.doc_sent_word_dict[d][m]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({27: 1, 125: 1, 526: 1, 1063: 1, 1258: 1, 1492: 1, 2691: 1})"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampler.wordCountSentence[d][m]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_count = sampler.wordCountSentence[d][m]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({27: 1, 125: 1, 526: 1, 1063: 1, 1258: 1, 1492: 1, 2691: 1})"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "secondFactor = (sampler.ns_dk[d, :] + sampler.alpha) / \\\n",
    "                       (sampler.ns_d[d] + sampler.numTopics * sampler.alpha)\n",
    "thirdFactor = (sampler.ns_dkl[d, :, :] + sampler.gamma) / \\\n",
    "                      (sampler.ns_dk[d] + sampler.numSentiments * sampler.gamma)[:, np.newaxis] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  9.83465702e-01,   1.51302416e-02],\n",
       "       [  7.80031201e-05,   7.80031201e-05],\n",
       "       [  7.80031201e-05,   7.80031201e-05],\n",
       "       [  7.80031201e-05,   7.80031201e-05],\n",
       "       [  7.80031201e-05,   7.80031201e-05],\n",
       "       [  7.80031201e-05,   7.80031201e-05],\n",
       "       [  7.80031201e-05,   7.80031201e-05],\n",
       "       [  7.80031201e-05,   7.80031201e-05],\n",
       "       [  7.80031201e-05,   7.80031201e-05],\n",
       "       [  7.80031201e-05,   7.80031201e-05]])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "secondFactor[:,np.newaxis] * thirdFactor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 2)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "thirdFactor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "beta0 = sampler.n_kl + sampler.beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "betaw = sampler.n_wkl[0,t,s] + sampler.beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sampling()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "prob = np.ones((sampler.numTopics, sampler.numSentiments))\n",
    "\n",
    "word_count = sampler.wordCountSentence[d][m]\n",
    "for t in range(sampler.numTopics):\n",
    "    for s in range(sampler.numSentiments):\n",
    "        beta0 = sampler.n_kl[t][s] + sampler.beta\n",
    "        m0 = 0\n",
    "        for word in word_count.keys():\n",
    "            betaw = sampler.n_wkl[word,t,s] + sampler.beta\n",
    "            cnt = word_count[word]\n",
    "            for i in range(cnt):\n",
    "                prob[t][s] *= (betaw+i) / (beta0 + m0)\n",
    "                m0 += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  2.87578284e-25,   1.35537510e-15],\n",
       "       [  1.35537510e-15,   1.35537510e-15],\n",
       "       [  1.35537510e-15,   1.35537510e-15],\n",
       "       [  1.35537510e-15,   1.35537510e-15],\n",
       "       [  1.35537510e-15,   1.35537510e-15],\n",
       "       [  1.35537510e-15,   1.35537510e-15],\n",
       "       [  1.35537510e-15,   1.35537510e-15],\n",
       "       [  1.35537510e-15,   1.35537510e-15],\n",
       "       [  1.35537510e-15,   1.35537510e-15],\n",
       "       [  1.60711003e-33,   1.35537510e-15]])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  5.50457010e+05,   1.00000000e-02],\n",
       "       [  1.00000000e-02,   1.00000000e-02],\n",
       "       [  1.00000000e-02,   1.00000000e-02],\n",
       "       [  1.00000000e-02,   1.00000000e-02],\n",
       "       [  1.00000000e-02,   1.00000000e-02],\n",
       "       [  1.00000000e-02,   1.00000000e-02],\n",
       "       [  1.00000000e-02,   1.00000000e-02],\n",
       "       [  1.00000000e-02,   1.00000000e-02],\n",
       "       [  1.00000000e-02,   1.00000000e-02],\n",
       "       [  4.81010000e+02,   1.00000000e-02]])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beta0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "probTable[ti][si] = (matrixSDT[si].getValue(docNo, ti) + this.alpha) / (sumDST[docNo][si] + this.sumAlpha)\n",
    "//\t\t\t\t\t\t* (matrixDS.getValue(docNo, si) + this.gammas[si]) / (sumDS[docNo] + this.sumGamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "for s in range(sampler.numSentiments):\n",
    "    for topic in range(sampler.numTopics):\n",
    "        expectTSW = 1:\n",
    "            for word in "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
